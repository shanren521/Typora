有监督学习：分类和回归

无监督学习：

能用分类不用聚类

1. 线性回归（Linear Regression）：用于建立连续数值的预测模型，基于特征之间的线性关系进行建模。
2. 逻辑回归（Logistic Regression）：用于建立分类模型，预测二元或多元类别的概率。
3. 决策树（Decision Trees）：使用树状结构进行分类和回归任务，通过一系列决策规则对数据进行分割。
4. 随机森林（Random Forests）：由多个决策树组成的集成学习模型，通过投票或平均预测结果来提高准确性和鲁棒性。
5. 支持向量机（Support Vector Machines，SVM）：用于分类和回归的监督学习模型，通过在特征空间中找到最优超平面来分割数据。
6. 朴素贝叶斯（Naive Bayes）：基于贝叶斯定理和特征条件独立性假设，用于分类和文本分类等任务。
7. K近邻算法（K-Nearest Neighbors，KNN）：基于实例之间的相似度进行分类或回归，通过选择最接近目标样本的K个最近邻来进行预测。
8. 神经网络（Neural Networks）：由多个神经元组成的网络结构，可以用于解决各种复杂的问题，如图像识别、自然语言处理等。
9. 卷积神经网络（Convolutional Neural Networks，CNN）：主要用于图像处理任务，通过卷积操作来提取图像特征。
10. 循环神经网络（Recurrent Neural Networks，RNN）：适用于序列数据处理，具有记忆能力，常用于语言建模、机器翻译等任务。

### 线性回归：有监督

线性回归可以通过最小二乘法来估计模型的参数，最小化观测值与预测值之间的平方误差和。一旦参数估计完成，我们可以使用模型来进行预测，并评估预测的准确性

1. 假设函数选择：线性回归假设输入特征与输出变量之间存在线性关系。它通过使用输入特征的线性组合来建立模型。假设函数的形式可以表示为：hθ(x) = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₙxₙ，其中 hθ(x) 表示预测值，θ₀, θ₁, θ₂, ..., θₙ 表示模型的参数，x₁, x₂, ..., xₙ 表示输入特征。
2. 参数估计：在线性回归中，我们需要估计模型的参数，即确定θ₀, θ₁, θ₂, ..., θₙ 的值。常用的方法是最小化损失函数，使得预测值与实际值之间的差异最小化。最常见的损失函数是均方误差（Mean Squared Error），即将预测值与实际值之间的差的平方进行求和并取平均。
3. 损失函数最小化：为了估计模型参数，我们需要选择合适的优化算法来最小化损失函数。最常用的优化算法是梯度下降法，它通过计算损失函数关于参数的梯度，并迭代更新参数，使损失函数逐步减小，从而找到最优的模型参数。
4. 模型评估：在训练完成后，我们需要对模型进行评估。常用的评估指标包括均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）等，这些指标可以帮助我们了解模型的性能和预测能力。

![image-20230601220643593](D:\learn\Typora\pictures\image-20230601220643593.png)

 ![image-20230601221216465](D:\learn\Typora\pictures\image-20230601221216465.png)

 

误差越小越好，误差是独立并且具有相同的分布，并且服从均值为0方差为θ的平方的高斯分布(正态分布)

![image-20230601222623733](D:\learn\Typora\pictures\image-20230601222623733.png)

![image-20230601223512745](D:\learn\Typora\pictures\image-20230601223512745.png)



![image-20230601223726439](D:\learn\Typora\pictures\image-20230601223726439.png)

J(θ)越小越好，结果越大越好。

![image-20230601224311440](D:\learn\Typora\pictures\image-20230601224311440.png)

![image-20230601225035063](D:\learn\Typora\pictures\image-20230601225035063.png)

梯度下降是一个优化算法，上面的矩阵的逆可能不成立，梯度是向上的，梯度的反方向就是梯度下降

梯度优化的步长不能多大，否则会导致梯度下降的方向不正确。

![image-20230601230154030](D:\learn\Typora\pictures\image-20230601230154030.png)

求偏导，小步长，不断更新，因为每个参数直接都是独立互不影响的，所以可以分开计算。

![image-20230601230338137](D:\learn\Typora\pictures\image-20230601230338137.png)

![image-20230601232121082](D:\learn\Typora\pictures\image-20230601232121082.png)

批量梯度下降：BGD，随机梯度下降：SGD，小批量梯度下降：MBGD 常见batch size  64 128 256

batch size一次迭代的样本数量

![image-20230601232503099](D:\learn\Typora\pictures\image-20230601232503099.png)

学习率常见为0.001、0.01

代码实现中只要实现一下公式就可以

![image-20230601233031449](D:\learn\Typora\pictures\image-20230601233031449.png)

样本个数，学习率，真实值，预测值

### 非线性回归

### 交叉验证

![image-20230602230536127](D:\learn\Typora\pictures\image-20230602230536127.png)

为了自我调节，训练数据和测试数据一般为8:2，从训练集中交叉获取8份当作训练集，剩余2份作为验证集

### 混淆矩阵

![image-20230603091532307](D:\learn\Typora\pictures\image-20230603091532307.png)

![image-20230603092726187](D:\learn\Typora\pictures\image-20230603092726187.png)

​		Precision：精确率(查准率)，recall：召回率(查全率)，这两个值是矛盾值，一个高一个就会低

![image-20230603093315152](D:\learn\Typora\pictures\image-20230603093315152.png)

### ROC曲线

![image-20230603095105167](D:\learn\Typora\pictures\image-20230603095105167.png)

![image-20230603102847170](D:\learn\Typora\pictures\image-20230603102847170.png)

![image-20230603103210114](D:\learn\Typora\pictures\image-20230603103210114.png)

线性回归中的问题：步长太小，步长太大。数据必须标准化

![image-20230603110725943](D:\learn\Typora\pictures\image-20230603110725943.png)

批量梯度下降：m=1，小批量梯度下降m=minbatch

```python
import numpy as np
import matplotlib.pyplot as plt
import warnings
from sklearn.linear_model import LinearRegression

plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12

warnings.filterwarnings('ignore')

X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 拼接
X_b = np.c_[np.ones((100, 1)), X]
# 求逆
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
print(theta_best)

X_new = np.array([[0], [2]])
print(X_new)
X_new_b = np.c_[np.ones((2, 1)), X_new]
y_predict = X_new_b.dot(theta_best)

lin_reg = LinearRegression()
lin_reg.fit(X, y)
print("", lin_reg.coef_)
print("", lin_reg.intercept_)

# plt.plot(X, y, 'b.')
# plt.xlabel('X_1')
# plt.ylabel('y')
# plt.axis([0, 2, 0, 15])
# plt.show()

# 批量梯度下降
eta = 0.1  # 学习率
n_iterations = 1000  # 迭代次数
m = 100  # 样本数量
theta = np.random.randn(2, 1)  #
for iteration in range(n_iterations):
    # 计算公式
    gradients = 2 / m * X_b.T.dot((X_b.dot(theta) - y))
    theta = theta - eta * gradients

res = X_new_b.dot(theta)
print(res)

# plt.plot(X_new, y_predict, 'r--')
# plt.plot(X, y, 'b.')
# plt.xlabel('X_1')
# plt.ylabel('y')
# plt.axis([0, 2, 0, 15])
# plt.show()

theta_path_bgd = []


def plot_gradient_descent(theta, eta, theta_path=None):
    m = len(X_b)
    plt.plot(X, y, 'b.')
    n_iterations = 1000
    theta = np.random.randn(2, 1)  #
    for iteration in range(n_iterations):
        y_predict = X_new_b.dot(theta)
        plt.plot(X_new, y_predict, 'g--')
        # 计算公式
        gradients = 2 / m * X_b.T.dot((X_b.dot(theta) - y))
        theta = theta - eta * gradients
        if theta_path is not None:
            theta_path_bgd.append(theta)
    plt.xlabel("X_1")
    plt.axis([0, 2, 0, 15])
    plt.title("eta = {}".format(eta))


theta = np.random.randn(2, 1)
plt.figure(figsize=(10, 4))
plt.subplot(131)
plot_gradient_descent(theta, eta=0.02)
plt.subplot(132)
plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)
plt.subplot(133)
plot_gradient_descent(theta, eta=0.5)
plt.show()
print("theta_path_bgd", theta_path_bgd)

theta_path_sgd = []
m = len(X_b)
n_epoch = 50
t0 = 5
t1 = 50

def learning_schedule(t):
    return t0 / (t1 + t)

# 随机梯度下降
theta = np.random.randn(2, 1)
for epoch in range(n_epoch):
    for i in range(m):
        if epoch < 10 and i < 10:
            y_predict = X_new_b.dot(theta)
            plt.plot(X_new, y_predict, 'r--')
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index+1]
        yi = y[random_index:random_index+1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(n_epoch*m+i)
        theta = theta -eta * gradients
        theta_path_sgd.append(theta)

plt.plot(X, y, 'b.')
plt.axis([0, 2, 0, 15])
plt.show()


# 小批量梯度下降
theta_path_mgd = []
n_epoch = 50
minbatch = 16  # 2的次幂
theta = np.random.randn(2, 1)
np.random.seed(0)  # 使随机结果相同
t = 0
for epoch in range(n_epoch):
    shuffled_indices = np.random.permutation(m)
    X_b_shuffled = X_b[shuffled_indices]
    y_shuffled = y[shuffled_indices]
    for i in range(0, m, minbatch):
        t += 1
        xi = X_b_shuffled[i:i + minbatch]
        yi = y_shuffled[i:i+minbatch]
        gradients = 2/minbatch * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(t)
        theta = theta - eta * gradients
        theta_path_mgd.append(theta)

# 3种策略的对比实验
theta_path_bgd = np.array(theta_path_bgd)
theta_path_sgd = np.array(theta_path_sgd)
theta_path_mgd = np.array(theta_path_mgd)

print("theta_path_bgd", theta_path_bgd)
plt.figure(figsize=(8, 4))
plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], 'r-s', linewidth=1, label='SGD')
plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], 'g-+', linewidth=2, label='MGD')
plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], 'b-o', linewidth=3, label='BGD')
plt.legend(loc='upper left')
plt.axis([3.5, 4.5, 2.0, 4.0])
plt.show()
```



### 多项式回归: 不建议degree过大

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X ** 2 + np.random.randn(m, 1)
plt.plot(X, y, 'b.')
plt.xlabel('X_1')
plt.ylabel('y')
plt.axis([-3, 3, -5, 10])
plt.show()
poly_feature = PolynomialFeatures(degree=2, include_bias=False)
# 数据会多平方的一列
X_poly = poly_feature.fit_transform(X)

lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
print(lin_reg.coef_)  # 权重参数
print(lin_reg.intercept_)  # 偏置项

X_new = np.linspace(-3, 3, 100).reshape(100, 1)
X_new_poly = poly_feature.transform(X_new)
y_new = lin_reg.predict(X_new_poly)
plt.plot(X, y, 'b.')
plt.plot(X_new, y_new, 'r--', label='prediction')
plt.axis([-3, 3, -5, 10])
plt.legend()
plt.show()
# 不同复杂度比较
for style, width, degree in (('g-', 1, 100), ('b--', 1, 2), ('r-+', 1, 1)):
    poly_feature = PolynomialFeatures(degree=degree, include_bias=False)
    std = StandardScaler()
    lin_reg = LinearRegression()
    polynomial_reg = Pipeline([('poly_feature', poly_feature),
                               ("StandardScaler", std),
                               ("lin_reg", lin_reg)
                               ])
    polynomial_reg.fit(X, y)
    y_new_2 = polynomial_reg.predict(X_new)
    plt.plot(X_new, y_new_2, label=str(degree), linewidth=width)

plt.plot(X, y, 'b.')
plt.axis([-3, 3, -5, 10])
plt.legend()
plt.show()
```



### 正则化

对权重参数进行惩罚，让权重参数尽可能平滑一些

![image-20230603172758039](D:\learn\Typora\pictures\image-20230603172758039.png)

正则化：解决过拟合，岭回归(Ridge)和lasso

过拟合：在训练集表现好，测试集表现差，欠拟合：在训练集和测试集表示都差

### 逻辑回归：分类算法，有监督

逻辑回归的基本思想是通过建立一个逻辑函数（也称为sigmoid函数），将输入的特征映射到0和1之间的概率值，用于表示样本属于某个类别的概率。

逻辑回归模型的训练过程是通过最大似然估计来估计模型的参数。最终的目标是使观测值所属类别的概率与实际标签之间的差异最小化。为了实现这一点，通常使用对数损失函数（也称为交叉熵损失函数）来度量模型预测值与真实标签之间的差异。

逻辑回归是一种线性分类器，只能用于解决线性可分问题。对于非线性问题，可以使用核技巧（如多项式核或高斯核）或将逻辑回归与其他非线性模型（如支持向量机）结合来处理。

##### 基本思想可以总结为以下几个步骤：

1. 假设函数选择：假设函数选择是逻辑回归算法的核心。逻辑回归假设输入特征与输出标签之间存在某种线性关系。它通过使用线性函数的线性组合，并将结果通过sigmoid函数映射到0和1之间的概率值来建立模型。
2. 参数估计：在逻辑回归中，我们需要估计模型的参数。常用的方法是最大似然估计，即通过最大化训练数据中观察到的标签概率的乘积来寻找最优参数。参数估计的过程通常使用梯度下降等优化算法来实现。
3. 损失函数选择：为了对模型进行参数估计，需要选择合适的损失函数来度量模型预测值与真实标签之间的差异。在逻辑回归中，常用的损失函数是对数损失函数（也称为交叉熵损失函数），它能够有效地度量概率预测的误差。
4. 决策边界确定：通过学习得到的模型参数，我们可以根据设定的阈值来进行分类决策。通常，当预测概率大于阈值时，我们将样本分为正类（例如1），当预测概率小于等于阈值时，我们将样本分为负类（例如0）。决策边界是由模型参数确定的，它将特征空间分割成两个不同的区域。
5. 模型评估：在训练完成后，我们需要对模型进行评估。常用的评估指标包括准确率、精确率、召回率、F1分数等，这些指标可以帮助我们了解模型的性能和预测能力。

![image-20230603221312130](D:\learn\Typora\pictures\image-20230603221312130.png)

![image-20230603221648881](D:\learn\Typora\pictures\image-20230603221648881.png)

z = x ** 2 + y ** 2

![image-20230603222144278](D:\learn\Typora\pictures\image-20230603222144278.png)

![image-20230603222519882](D:\learn\Typora\pictures\image-20230603222519882.png)

![image-20230603222733539](D:\learn\Typora\pictures\image-20230603222733539.png)

![image-20230603222839369](D:\learn\Typora\pictures\image-20230603222839369.png)

##### softmax可以进行多分类任务

##### 交叉熵损失函数

![image-20230604160953963](D:\learn\Typora\pictures\image-20230604160953963.png)

当前特征类别为1，其他类别特征为0

#### 决策边界的绘制

+ 构建坐标数据
+ 整合坐标点，得到所有测试输入数据坐标点
+ 预测，得到所有点的概率值
+ 绘制等高线

### K-MEANS算法：无监督

```python
from sklearn.cluster import KMeans
```

K-Means算法的基本思想如下：

1. 选择K个初始的聚类中心点，可以是随机选择或通过其他方法初始化。
2. 将每个数据点分配给与其最近的聚类中心点，形成K个簇。
3. 对于每个簇，计算其质心（即聚类中心），即簇内所有数据点的平均值。
4. 将每个数据点重新分配给最近的质心。
5. 重复步骤3和4，直到簇不再改变或达到最大迭代次数。

**K-means++**：K-means++是一种改进的初始中心点选取方法，旨在更好地初始化聚类过程。该方法首先随机选择一个数据点作为第一个中心点，然后按照距离当前已选中中心点最远的原则，选择下一个中心点，重复该过程直到选取K个中心点。

K-Means算法的一些注意事项:

- K的选择：K值的选择是一个重要的问题，通常需要通过试验和评估指标来确定最佳的K值。
- 初始聚类中心的选择：初始聚类中心的选择可以对最终的聚类结果产生影响，不同的初始化方法可能会导致不同的结果。
- 对异常值的敏感性：K-Means算法对异常值敏感，异常值可能会影响聚类中心的计算和簇的划分。
- 收敛性和局部最优：K-Means算法可能会陷入局部最优解，无法得到全局最优解。因此，多次运行算法并选择最佳结果是一种常见的策略。

![image-20230604173023461](D:\learn\Typora\pictures\image-20230604173023461.png)

![image-20230604173336265](D:\learn\Typora\pictures\image-20230604173336265.png)

![image-20230604174728024](D:\learn\Typora\pictures\image-20230604174728024.png)

![image-20230604174928908](D:\learn\Typora\pictures\image-20230604174928908.png)

![image-20230604182420354](D:\learn\Typora\pictures\image-20230604182420354.png)

![image-20230604182521878](D:\learn\Typora\pictures\image-20230604182521878.png)

![image-20230604183430861](D:\learn\Typora\pictures\image-20230604183430861.png)

![image-20230604183456148](D:\learn\Typora\pictures\image-20230604183456148.png)

![image-20230604183851853](D:\learn\Typora\pictures\image-20230604183851853.png)

https://www.naftaliharris.com/blog/visualizing-k-means-clustering/   

K-Means：

+ 决策边界
+ 算法流程
+ 不稳定结果
+ 评估指标：inertia每个样本与其质心的距离的平方和
+ transform得到每个样本到其质心的距离
+ 找到最佳簇数：如果k值越大，评估值肯定会越来越小。执行多个建模，查找拐点。
+ ![image-20230604220330688](D:\learn\Typora\pictures\image-20230604220330688.png)
+ kmeans存在的问题：每次得到的结果是不同的，根初始化结果有关系。评估值低不一定是最好的。

### 决策树

![image-20230604231955019](D:\learn\Typora\pictures\image-20230604231955019.png)

![image-20230604232403950](D:\learn\Typora\pictures\image-20230604232403950.png)

![image-20230604232522948](D:\learn\Typora\pictures\image-20230604232522948.png)

![image-20230604232625766](D:\learn\Typora\pictures\image-20230604232625766.png)

![image-20230604232941754](D:\learn\Typora\pictures\image-20230604232941754.png)

![image-20230606212425253](D:\learn\Typora\pictures\image-20230606212425253.png)

**选取根节点，根据信息增益计算出那个节点的熵值小选择哪个作为根节点**

同样的方式可以计算出其他特征的信息增益，那么我们选择最大的哪个就可以，进行迭代操作，直到最后一个节点

![image-20230606213636699](D:\learn\Typora\pictures\image-20230606213636699.png)

![image-20230606214636493](D:\learn\Typora\pictures\image-20230606214636493.png)

![image-20230606215208798](D:\learn\Typora\pictures\image-20230606215208798.png)

决策树做分类任务的时候，使用方差计算，方差越小越好。

#### 决策树中的正则化

DecisionTreeClassifier类还有一些其他参数类似的限制了决策树的形状：

min_samples_split节点在分割之前必须具有的最小样本数

min_samples_leaf叶子节点必须具有的最小样本数

max_leaf_nodes叶子节点的最大数量

max_features在每个节点处评估用于拆分的最大特征数

### 集成算法

![image-20230607212130687](D:\learn\Typora\pictures\image-20230607212130687.png)

![image-20230607212502619](D:\learn\Typora\pictures\image-20230607212502619.png)

![image-20230607214958056](D:\learn\Typora\pictures\image-20230607214958056.png)

##### 随机森林的优势

![image-20230607215043239](D:\learn\Typora\pictures\image-20230607215043239.png)

随机森林：**可解释性非常强**

##### 随机森林的基本思想可以总结为以下几个步骤：

1. 数据随机采样：对于给定的训练数据集，随机森林通过有放回地从中进行随机采样，形成多个不同的训练子集。这种采样方式被称为自助采样法（bootstrap sampling），它确保每个子集都具有一定的差异性。
2. 决策树的构建：对于每个采样的子集，随机森林使用决策树算法来构建一棵决策树。决策树是一种基于特征条件进行分层划分的树状结构，它可以根据特征的取值来对样本进行分类或回归。
3. 特征随机选择：在构建每棵决策树的过程中，随机森林对于每个划分节点的特征选择都是随机的。通常，对于每个划分节点，从所有特征中随机选择一部分特征作为候选特征，然后从中选择最优的特征进行划分。
4. 多个决策树的集成：随机森林通过构建多棵决策树并将它们集成起来，形成一个强大的集成模型。在分类任务中，随机森林使用投票或概率平均的方式来确定最终的预测类别。在回归任务中，随机森林使用平均值来确定最终的预测输出。

随机森林的优点包括：

- 可以处理高维数据和大量特征，不需要对数据进行特征选择。
- 具有较好的鲁棒性，对于缺失值和异常值具有一定的容忍性。
- 能够评估特征的重要性，帮助了解哪些特征对于预测最重要。
- 可以有效地处理大规模数据集，具有并行化计算的能力。

理论上树越多效果越好，实际上超过一定数量就是上下浮动了。

![image-20230607215936977](D:\learn\Typora\pictures\image-20230607215936977.png)

#### 提升算法(boosting)

![image-20230607220722082](D:\learn\Typora\pictures\image-20230607220722082.png)

![image-20230607220737185](D:\learn\Typora\pictures\image-20230607220737185.png)

![image-20230607220750084](D:\learn\Typora\pictures\image-20230607220750084.png)

### 支持向量机

![image-20230608201205080](D:\learn\Typora\pictures\image-20230608201205080.png)

![image-20230608211353756](D:\learn\Typora\pictures\image-20230608211353756.png)

![image-20230608211644271](D:\learn\Typora\pictures\image-20230608211644271.png)

![image-20230608211948644](D:\learn\Typora\pictures\image-20230608211948644.png)

点到平面的距离

![image-20230608212128876](D:\learn\Typora\pictures\image-20230608212128876.png)

![image-20230608212426484](D:\learn\Typora\pictures\image-20230608212426484.png)

![image-20230608212624888](D:\learn\Typora\pictures\image-20230608212624888.png)

![image-20230608213037644](D:\learn\Typora\pictures\image-20230608213037644.png)

![image-20230608213408440](D:\learn\Typora\pictures\image-20230608213408440.png)

![image-20230608213459692](D:\learn\Typora\pictures\image-20230608213459692.png)

![image-20230608213749100](D:\learn\Typora\pictures\image-20230608213749100.png)

![image-20230608214125756](D:\learn\Typora\pictures\image-20230608214125756.png)

![image-20230608214133449](D:\learn\Typora\pictures\image-20230608214133449.png)

![image-20230608214312577](D:\learn\Typora\pictures\image-20230608214312577.png)

![image-20230608214557607](D:\learn\Typora\pictures\image-20230608214557607.png)

![image-20230608214840773](D:\learn\Typora\pictures\image-20230608214840773.png)

**边界上的点为支持向量的点，其他点的α为0**

![image-20230608215823528](D:\learn\Typora\pictures\image-20230608215823528.png)

![image-20230608220037738](D:\learn\Typora\pictures\image-20230608220037738.png)

![image-20230608220539088](D:\learn\Typora\pictures\image-20230608220539088.png)

![image-20230608220558849](D:\learn\Typora\pictures\image-20230608220558849.png)

![image-20230608220608772](D:\learn\Typora\pictures\image-20230608220608772.png)

![image-20230608220629449](D:\learn\Typora\pictures\image-20230608220629449.png)

![image-20230608220716488](D:\learn\Typora\pictures\image-20230608220716488.png)

![image-20230608221552351](D:\learn\Typora\pictures\image-20230608221552351.png)

高斯核函数

![image-20230609195201585](D:\learn\Typora\pictures\image-20230609195201585.png)

![image-20230609200427020](D:\learn\Typora\pictures\image-20230609200427020.png)

![image-20230609200702046](D:\learn\Typora\pictures\image-20230609200702046.png)

![image-20230609201001263](D:\learn\Typora\pictures\image-20230609201001263.png)

![image-20230609201044754](D:\learn\Typora\pictures\image-20230609201044754.png)

![image-20230609201054810](D:\learn\Typora\pictures\image-20230609201054810.png)

![image-20230609202400108](D:\learn\Typora\pictures\image-20230609202400108.png)

![image-20230609202532349](D:\learn\Typora\pictures\image-20230609202532349.png)

![image-20230609202630268](D:\learn\Typora\pictures\image-20230609202630268.png)

![image-20230609202753570](D:\learn\Typora\pictures\image-20230609202753570.png)

![image-20230609202850112](D:\learn\Typora\pictures\image-20230609202850112.png)

![image-20230609203021893](D:\learn\Typora\pictures\image-20230609203021893.png)

![image-20230609203037203](D:\learn\Typora\pictures\image-20230609203037203.png)

![image-20230609203722895](D:\learn\Typora\pictures\image-20230609203722895.png)

![image-20230609203952612](D:\learn\Typora\pictures\image-20230609203952612.png)

![image-20230609204033343](D:\learn\Typora\pictures\image-20230609204033343.png)

![image-20230609204139745](D:\learn\Typora\pictures\image-20230609204139745.png)

![image-20230609204240677](D:\learn\Typora\pictures\image-20230609204240677.png)

![image-20230609204938215](D:\learn\Typora\pictures\image-20230609204938215.png)

![image-20230609205539182](D:\learn\Typora\pictures\image-20230609205539182.png)

![image-20230609214308688](D:\learn\Typora\pictures\image-20230609214308688.png)



![image-20230609214320553](D:\learn\Typora\pictures\image-20230609214320553.png)

![image-20230609214822279](D:\learn\Typora\pictures\image-20230609214822279.png)

![image-20230609221211325](D:\learn\Typora\pictures\image-20230609221211325.png)



![image-20230609221344977](D:\learn\Typora\pictures\image-20230609221344977.png)

![image-20230609221714876](D:\learn\Typora\pictures\image-20230609221714876.png)

![image-20230609221738443](D:\learn\Typora\pictures\image-20230609221738443.png)

![image-20230609221748859](D:\learn\Typora\pictures\image-20230609221748859.png)

![image-20230609221928446](D:\learn\Typora\pictures\image-20230609221928446.png)

![image-20230609222045355](D:\learn\Typora\pictures\image-20230609222045355.png)

![image-20230609223231746](D:\learn\Typora\pictures\image-20230609223231746.png)

神经元越多，速度越慢，越容易过拟合，效果也越好，神经元常见64、128、256、512

![image-20230609224238366](D:\learn\Typora\pictures\image-20230609224238366.png)

![image-20230609224306971](D:\learn\Typora\pictures\image-20230609224306971.png)

![image-20230609224549172](D:\learn\Typora\pictures\image-20230609224549172.png)

Sigmoid容易出现梯度消失现象，ReLU在小于0的方向会出现无法激活的现象

![image-20230609224653838](D:\learn\Typora\pictures\image-20230609224653838.png)

![image-20230609224831306](D:\learn\Typora\pictures\image-20230609224831306.png)

dropout在训练的过程中在每一层随机的杀死一部分神经元

![image-20230610101144854](D:\learn\Typora\pictures\image-20230610101144854.png)

![image-20230610101413679](D:\learn\Typora\pictures\image-20230610101413679.png)

![image-20230610101801170](D:\learn\Typora\pictures\image-20230610101801170.png)

![image-20230610101958713](D:\learn\Typora\pictures\image-20230610101958713.png)

![](D:\learn\Typora\pictures\image-20230610102339595.png)

![image-20230610103444941](D:\learn\Typora\pictures\image-20230610103444941.png)

![image-20230610103704556](D:\learn\Typora\pictures\image-20230610103704556.png)

![image-20230610110508762](D:\learn\Typora\pictures\image-20230610110508762.png)

![image-20230610110612108](D:\learn\Typora\pictures\image-20230610110612108.png)

![image-20230610110751897](D:\learn\Typora\pictures\image-20230610110751897.png)

![image-20230610110854286](D:\learn\Typora\pictures\image-20230610110854286.png)

![image-20230610111010943](D:\learn\Typora\pictures\image-20230610111010943.png)

![image-20230610111520907](D:\learn\Typora\pictures\image-20230610111520907.png)

![image-20230610111624014](D:\learn\Typora\pictures\image-20230610111624014.png)

提升度大于1表示有关联

![image-20230610112429358](D:\learn\Typora\pictures\image-20230610112429358.png)

![image-20230610112629264](D:\learn\Typora\pictures\image-20230610112629264.png)

![image-20230610181222693](D:\learn\Typora\pictures\image-20230610181222693.png)

如果一个项集是频繁项集，那么它的子集也都是频繁项集

如果一个项集是非频繁项集，那么它的所有超集也是非频繁的

![image-20230610181636706](D:\learn\Typora\pictures\image-20230610181636706.png)

![image-20230610210712993](D:\learn\Typora\pictures\image-20230610210712993.png)

![image-20230610213938022](D:\learn\Typora\pictures\image-20230610213938022.png)

![image-20230610214922222](D:\learn\Typora\pictures\image-20230610214922222.png)

滑动窗口一般都是奇数

![image-20230610215119958](D:\learn\Typora\pictures\image-20230610215119958.png)

![image-20230610215415918](D:\learn\Typora\pictures\image-20230610215415918.png)

![image-20230610215735239](D:\learn\Typora\pictures\image-20230610215735239.png)

![image-20230610215918329](D:\learn\Typora\pictures\image-20230610215918329.png)

![image-20230610215942946](D:\learn\Typora\pictures\image-20230610215942946.png)

![image-20230610220023439](D:\learn\Typora\pictures\image-20230610220023439.png)







