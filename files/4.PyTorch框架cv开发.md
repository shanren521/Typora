### 1.numpy基础

```python
import numpy as np

a = np.array([1, 2, 3, 4, 5, 6])
b = np.array([8, 7, 6, 5, 4, 3])
print(a)
print(a.shape)
c = np.reshape(a, (3, 2))
print(c)
print(c.shape)
d = np.reshape(b, (1, 1, 2, 3))
print(d, d.shape)
e = np.squeeze(d)
print(e, e.shape)
f = e.transpose((1, 0))
print(f)
index = np.argmax(f)
print(index, f[index])

arr = np.zeros((6, 6), np.uint8)
arr2 = np.linspace((6, 4, 2), 10, 10)
print(arr)
print(arr2)
```

### 2.opencv基础

```python
import cv2 as cv
import numpy as np

image = cv.imread("D:/images/Lenna.png")

cv.imshow("input", image)
h, w, c = image.shape
print(h, w, c)

blob = np.transpose(image, (2, 0, 1))
print(blob.shape)

fi = np.float32(image) / 255.0  # 0-1
cv.imshow("fi", fi)

gray = cv.cvtColor(image, cv.COLOR_BGRA2GRAY)
cv.imshow("gray", gray)

dst = cv.resize(image, (256, 256))
cv.imshow("zoom out", dst)

r = image[:, :, 0]
g = image[:, :, 1]
b = image[:, :, 2]

box = [200, 100, 200, 200]  # x, y, w, h
roi = image[200:400, 100:300, :]
cv.imshow("roi", roi)

m1 = np.zeros((256, 256, 3), np.uint8)
m1[:, :] = (127, 0, 127)
cv.imshow("m1", m1)
cv.rectangle(image, (200, 200), (400, 400), (127, 127, 0), 3, 8)
cv.imshow("input", image)

cap = cv.VideoCapture("D:/images/lane.avi")
while True:
    ret, frame = cap.read()
    if not ret:
        break
    cv.imshow("frame", frame)

    c = cv.waitKey(5)
    if c == ord("q"):
        break

cv.waitKey(0)
cv.destroyAllWindows()
```

### 3.PyTorch基础

```python
import torch

# 生成空的tensor
x = torch.empty(2, 2)
# 生成满足正态分布(0-1)的随机数
x = torch.randn(2, 2)
# 生成0-1之间的随机数
x = torch.rand(2, 2)
# 初始化为0
x = torch.zeros(2, 2)
# 自定义tensor
x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
y = torch.tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])
# tensor相加
z = x.add(y)
# 维度变换
x = x.view(-1, 3)
# tensor数据转为numpy
nx = x.numpy()
# numpy数据转为tensor
x = torch.from_numpy(nx)
# 查看是否有GPU
if torch.cuda.is_available():
    print("GPU Detected!")
    # 使用GPU计算
    result = x.view(-1).cuda() + y.cuda()
    print(result)
print(x, x.size())
```

#### 4.自动梯度与回归

```python
import torch

# https://deeplizard.com/learn/video/Csa5R12jYRg
# requires_grad 是否自动求导
x = torch.randn(1, 5, requires_grad=True)
y = torch.randn(5, 3, requires_grad=True)
z = torch.randn(3, 1, requires_grad=True)
print("x: \n", x, "\ny: \n", y, "\nz: \n", z)
# 矩阵相乘
xy = torch.matmul(x, y)
print("xy: \n", xy)
xyz = torch.matmul(xy, z)
print("xyz: ", xyz)
# 实现反向求导操作
xyz.backward()
# z的梯度 z.grad == xy == xyz对z求导
print(x.grad, y.grad, z.grad)

# 线性回归
import numpy as np
import torch
from torch import nn
import matplotlib.pyplot as plt

x = np.array([1, 2, 0.5, 2.5, 2.6, 3.1], np.float32).reshape((-1, 1))
y = np.array([3.7, 4.6, 1.65, 5.68, 5.98, 6.95], np.float32).reshape((-1, 1))


class LinearRegressionModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        out = self.linear(x)
        return out


input_dim = 1
output_dim = 1
# 模型
model = LinearRegressionModel(input_dim, output_dim)
# 损失函数
criterion = nn.MSELoss()

learning_rate = 0.01
# 优化器
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

for epoch in range(100):
    epoch += 1
    inputs = torch.from_numpy(x).requires_grad_()
    labels = torch.from_numpy(y)
    # 梯度清零
    optimizer.zero_grad()
    # 调用forward函数
    outputs = model(inputs)
    # 计算损失
    loss = criterion(outputs, labels)
    # 反向梯度
    loss.backward()
    # 通过优化器，更新参数
    optimizer.step()
    print("epoch {}, loss {}".format(epoch, loss.item()))

predicted_y = model(torch.from_numpy(x).requires_grad_()).data.numpy()
print("标签Y: ", y)
print("预测Y： ", predicted_y)

plt.clf()
predicted = model(torch.from_numpy(x).requires_grad_()).data.numpy()
# 绘制真实值
plt.plot(x, y, "go", label="True data", alpha=0.5)
# 绘制预测值
plt.plot(x, predicted_y, "--", label="Predictions", alpha=0.5)

plt.legend(loc="best")
plt.show()

# 逻辑回归
import torch
import numpy as np
from torch import nn
import matplotlib.pyplot as plt


x = np.linspace(-5, 5, 20, np.float32)
_b = 1 / (1 + np.exp(-x))
y = np.random.normal(_b, 0.005)

x = np.float32(x.reshape((-1, 1)))
y = np.float32(y.reshape((-1, 1)))

class LogicRegressionModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(LogicRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        out = torch.sigmoid(self.linear(x))
        return out

input_dim = 1
output_dim = 1
# sigmoid+BCE”对应的是torch.nn.BCEWithLogitsLoss，而“softmax+CE”对应的是torch.nn.CrossEntropyLoss
# 模型
model = LogicRegressionModel(input_dim, output_dim)
# 损失函数
criterion = torch.nn.BCELoss()
# 学习率
learning_rate = 0.01
# 优化器
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

# sigmoid -> 1 / (1 + e^(-x))
for epoch in range(100):
    epoch += 1
    inputs = torch.from_numpy(x).requires_grad_()
    labels = torch.from_numpy(y)
    # 梯度清零
    optimizer.zero_grad()
    # 模型预测, 调用forward函数
    predict = model(inputs)
    # 计算损失
    loss = criterion(predict, labels)
    # 反向梯度
    loss.backward()
    # 更新参数
    optimizer.step()
    print("epoch {}, loss {}".format(epoch, loss.item()))

# 进行预测
predicted = model(torch.from_numpy(x).requires_grad_()).data.numpy()
print("标签Y: ", y)
print("预测Y: ", predicted)

plt.clf()
predicted_ = model(torch.from_numpy(x).requires_grad_()).data.numpy()
plt.plot(x, y, "go", label="True data", alpha=0.5)
plt.plot(x, predicted, "--", label="Predict", alpha=0.5)
plt.legend(loc="best")
plt.show()
```

### 5.人工神经网络

```python
import torch
import numpy as np
import cv2 as cv
import torchvision as tv
from torch.utils.data import DataLoader
from torch import nn
import matplotlib.pyplot as plt


'''
感知器
Sigmoid   ReLU
多层感知器
反向传播算法
    随机/世故梯度下降(online)
        基于单个样本、计算速度快、很容易收敛
    标准梯度下降(batch)
        基于整个数据集、计算速度慢、不容易收敛
    Mini-batch梯度下降
        小批量梯度下降、收敛速度快、算法精度高
常见数据集
    Pascal VOC
    COCO
基础数据集介绍
    torchvision.datasets包
    Mnist/Fashion-Mnist/CIFAR
    ImageNet/Pascal VOC/MS-COCO
    Cityscapes/FakeData
数据集的读取与加载
    torch.utils.data.Dataset的子集
    torch.utils.data.DataLoader加载数据集
模型训练
    超参数设置(批次(batch_size)/学习率(lr))
    优化器选择
    训练epoch/step  每训练batch_size个数据为一个step
'''
# ToTensor()把数据转为tensor类型，并且把数据转为0-1之间
# Normalize数据标准化，均值为0，方差为1，先计算出通道数据的均值与方差，然后将每个通道内的每个数据减去均值
# 再除以方差，得到归一化的结果
transform = tv.transforms.Compose([tv.transforms.ToTensor(),
                                   tv.transforms.Normalize((0.5,), (0.5,))])
# 手写数字识别数据集
train_ts = tv.datasets.MNIST(root="D:/datasets", train=True, download=True, transform=transform)
test_ts = tv.datasets.MNIST(root="D:/datasets", train=False, download=True, transform=transform)
train_dl = DataLoader(train_ts, batch_size=32, shuffle=True, drop_last=False)
test_dl = DataLoader(test_ts, batch_size=64, shuffle=True, drop_last=False)

# 简单定义神经网络模型
model = nn.Sequential(
    nn.Linear(784, 100),
    nn.ReLU(),
    nn.Linear(100, 10),
    nn.LogSoftmax(dim=1)
)


def train_mnist():
    # 负对数似然损失 主对角线的和 / 主对角线元素的个数
    # NLLLoss  LogSoftmax
    loss_fn = nn.NLLLoss(reduction="mean")
    # 自适应优化
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    # 训练
    for s in range(5):
        print("run in epoch: %d" % s)
        for i, (x_train, y_train) in enumerate(train_dl):
            x_train = x_train.view(x_train.shape[0], -1)
            y_pred = model(x_train)
            train_loss = loss_fn(y_pred, y_train)
            if (i + 1) % 100 == 0:
                print(i + 1, train_loss.item())
            model.zero_grad()
            train_loss.backward()
            optimizer.step()
    # 预测
    total = 0
    correct_count = 0
    for test_images, test_labels in test_dl:
        for i in range(len(test_labels)):
            image = test_images[i].view(1, 784)
            with torch.no_grad():
                pred_labels = model(image)
            plabels = torch.exp(pred_labels)
            probs = list(plabels.numpy()[0])
            pred_label = probs.index(max(probs))
            true_label = test_labels.numpy()[i]
            if pred_label == true_label:
                correct_count += 1

            total += 1
    print("total acc: %.2f" % (correct_count / total))
    # 保存整个模型   使用torch.load加载
    # 保存推理模型 torch.save(model.state_dict(), "D:/models/nn_mnist_model.pt")
    # 加载推理模型 model.load_state_dict(torch.load())
    # model.eval() 预测/验证时使用，model.train() 训练时使用
    # torch.save(model, "D:/models/nn_mnist_model.pt")
    torch.save(model.state_dict(), "D:/models/nn_mnist_state_dict_model.pt")


if __name__ == '__main__':
    # train_mnist()
    print("Model's state_dict: ")
    for param_tensor in model.state_dict():
        print(param_tensor, "\t", model.state_dict()[param_tensor].size())

    model.load_state_dict(torch.load("D:/models/nn_mnist_state_dict_model.pt"))
    model.eval()  # dropout bn
    image = cv.imread("D:/images/9_99.png", cv.IMREAD_GRAYSCALE)
    cv.imshow("input", image)
    # 必须和训练时生成的数据步骤保持一致
    img_f = np.float32(image) / 255.0 - 0.5
    img_f = img_f / 0.5
    img_f = np.reshape(img_f, (1, 784))
    pred_labels = model(torch.from_numpy(img_f))
    # 由于计算过程中使用的是LogSoftmax   log = ln, 这里需要换算回去
    plabels = torch.exp(pred_labels)
    # detach 把返回值从函数中切割出来，当作常量
    probs = list(plabels.detach().numpy()[0])
    pred_label = probs.index(max(probs))
    print("predict digit number: ", pred_label)
    cv.waitKey(0)
    cv.destroyAllWindows()
```

### 5.卷积神经网络

```python
'''
卷积神将网络的计算公式为：
N=(W-F+2P)/S+1
其中N：输出大小
W：输入大小
F：卷积核大小
P：填充值的大小
S：步长大小
卷积核/操作数/filter
卷积的锚点位置(默认是中心位置)
卷积的边缘填充方式valid/same  输入为5，卷积核3*3，输出为3，则是valid， 输出为5则是same
卷积神经网络的好处：
	共享权重
	像素迁移
	空间信息提取
池化层：
	均值池化
	最大值池化
卷积层特点：
	局部感受野
	权重共享机制
	池化下采样操作
	获取了图像的迁移、形变与尺度空间不变性特征
定义损失与优化器
	Adam优化器
	SGD优化器
'''
# CNN构建手写数字识别网络
import torch
import os
import torchvision as tv
from torch.utils.data import DataLoader
from torch import nn

os.environ["CUDA_VISIBLE_DEVICE"] = '1'
transform = tv.transforms.Compose([tv.transforms.ToTensor(),
                                   tv.transforms.Normalize((0.5,), (0.5,))])

train_ts = tv.datasets.MNIST(root="D:/datasets", train=True, download=True, transform=transform)
test_ts = tv.datasets.MNIST(root="D:/datasets", train=False, download=True, transform=transform)
train_dl = DataLoader(train_ts, batch_size=32, shuffle=True, drop_last=False)
test_dl = DataLoader(test_ts, batch_size=32, shuffle=True, drop_last=False)


class CNN_MNIST(nn.Module):
    def __init__(self):
        super(CNN_MNIST, self).__init__()
        self.cnn_layers = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1, stride=1),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.ReLU(),
            nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3, padding=1, stride=1),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.ReLU(),
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(7*7*32, 200),
            nn.ReLU(),
            nn.Linear(200, 100),
            nn.ReLU(),
            nn.Linear(100, 10),
            nn.LogSoftmax(dim=1)
        )

    def forward(self, x):
        out = self.cnn_layers(x)
        out = out.view(-1, 7*7*32)
        out = self.fc_layers(out)
        return out

model = CNN_MNIST()
model.to(device)
print("CNN_MNIST STATE_DICT: ")
for parameter in model.state_dict():
    print(parameter)

loss = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

for s in range(5):
    print("run in epoch : %d" % s)
    for i, (x_train, y_train) in enumerate(train_dl):
        x_train = x_train.cuda()
        y_train = y_train.cuda()
        y_pred = model.forward(x_train)
        train_loss = loss(y_pred, y_train)
        if(i + 1) % 100 == 0:
            print(i+1, train_loss.item())

        optimizer.zero_grad()
        train_loss.backward()
        optimizer.step()

torch.save(model.state_dict(), "D:/models/cnn_mnist_model.pt")
model.eval()

total = 0
correct_count = 0
for test_images, test_labels in test_dl:
    pred_labels = model(test_images.cuda())
    predicted = torch.max(pred_labels, 1)[1]
    correct_count += int((predicted == test_labels.cuda()).sum())
    total += len(test_labels)

print("准确度: %.2f", correct_count / total)

# ONNX格式模型导出与推理
import torch
from cnn_mnist import CNN_MNIST
import pyttsx3

engine = pyttsx3.init()
engine.say("9")
engine.runAndWait()

# pyttsx3语音包
# 将模型转为onnx模式
def demo():
    model = CNN_MNIST()

    model.load_state_dict(torch.load("D:/models/cnn_mnist_model.pt"))
    model.eval()

    dummy_input1 = torch.randn(1, 1, 28, 28)
    torch.onnx.export(model, (dummy_input1), "cnn_mnist.onnx", verbose=True)
```

### 6.PyTorch数据集与训练可视化

```python
'''
数据集类
	torch.utils.data   Dataset数据抽象类
	支持Map-style与Iterable-style
	Map-style完成方法：
		__getitem__()
		__len__()
数据集预处理
	torchvision.transform
	torch.ToTensor将数据转为tensor并且0-1之间
可视化
	Tensorboard
		访问http://localhost:6006
	Visdom
'''
import cv2
import torch
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torchvision import transforms


class FaceLandmarksDataset(Dataset):
    def __init__(self, txt_file):
        self.transform = transforms.Compose([transforms.ToTensor()])
        lines = []
        with open(txt_file) as read_file:
            for line in read_file:
                line = line.replace("\n", "")
                lines.append(line)
        self.landmarks_frame = lines

    def __len__(self):
        return len(self.landmarks_frame)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        contents = self.landmarks_frame[idx].split("\t")
        image_path = contents[0]
        img = cv2.imread(image_path)
        h, w, c = img.shape
        img = cv2.resize(img, (64, 64))
        img = (np.float32(img) / 255.0 - 0.5) / 0.5
        landmarks = np.zeros(10, dtype=np.float32)
        for i in range(1, len(contents), 2):
            landmarks[i-1] = np.float32(contents[i]) / w
            landmarks[i-1] = np.float32(contents[i+1]) / h
        landmarks = landmarks.astype("float32").reshape(-1, 2)
        img = img.transpose((2, 0, 1))
        sample = {"image": torch.from_numpy(img), "landmarks": torch.from_numpy(landmarks)}
        return sample


if __name__ == '__main__':
    ds = FaceLandmarksDataset("")
    for i in range(len(ds)):
        sample = ds[i]
        print(i, sample["image"].size(), sample["landmarks"].size())
        if i == 3:
            break
    dataloader = DataLoader(ds, batch_size=4, shuffle=True)
    for i_batch, sample_batched in enumerate(dataloader):
        print(i_batch, sample_batched["image"].size(), sample_batched["landmarks"].size())

# 可视化
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter("D:/pytorch_visual/cnn_mnist")
dataiter = iter(train_dl)
image, labels = dataiter.next()
img_grid = tv.utils.make_grid(image)
writer.add_image("four_fashion_mnist_image", img_grid)
writer.add_graph(model, image.cuda())
```

### 7.实战人脸识别

```python
'''
人脸五点标定数据与制作
全局最大池化：torch.nn.AdaptiveAvgPool2d((1,1))
全局均值池化：torch.nn.AdaptiveMaxPool2d((1,1))
全局深度池化：自定义
'''

```

### 8.实战多任务网络实现年龄与性别预测

```python
'''
UTKFace数据集 https://susanqq.github.io/UTKFace/
[age]_[gender]_[race]_[date&time].jpg
[age] is an integer from 0 to 116, indicating the age
[gender] is either 0 (male) or 1 (female)
[race] is an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern).
[date&time] is in the format of yyyymmddHHMMSSFFF, showing the date and time an image was collected to UTKFace
多任务网络设计：损失函数构成

'''
# 数据集
import os
import cv2 as cv
import torch
import numpy as np
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torchvision import transforms

max_age = 116.0


class AgeGenderDataset(Dataset):
    def __init__(self, root_dir):
        # self.transform = transforms.Compose([transforms.ToTensor(),
        #                                      transforms.Normalize(mean=[0.5,], std=[0.5,]),
        #                                      transforms.Resize((64, 64)),
        #                                      ])
        self.transform = transforms.Compose([transforms.ToTensor()])
        img_files = os.listdir(root_dir)
        nums = len(img_files)
        # 0-116岁
        self.ages = []
        # 0 male  1 female
        self.genders = []
        self.images = []
        index = 0
        for file_name in img_files:
            age_gender_group = file_name.split("_")
            age_ = age_gender_group[0]
            gender_ = age_gender_group[1]
            self.genders.append(np.float32(gender_))
            # 将年龄转化为0-1之间的值
            self.ages.append(np.float32(age_)/ np.float32(max_age))
            self.images.append(os.path.join(root_dir, file_name))
            index += 1
    def __len__(self):
        return len(self.images)

    def num_of_sample(self):
        return len(self.images)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            image_path = self.images[idx]
        else:
            image_path = self.images[idx]
        img = cv.imread(image_path)  # BGR
        # h, w, c = img.shape
        img = cv.resize(img, (64, 64))
        img = (np.float32(img) / 255.0 - 0.5) / 0.5
        # HWC to CHW
        img = img.transpose((2, 0, 1))
        sample = {"image": torch.from_numpy(img), "age": self.ages[idx], "gender": self.genders[idx]}
        return sample


if __name__ == '__main__':
    ds = AgeGenderDataset("D:/datasets/UTKFace")
    for i in range(len(ds)):
        sample = ds[i]
        print(i, sample["image"].size(), sample["age"])
        if i == 3:
            break
    dataloader = DataLoader(ds, batch_size=4, shuffle=True, num_workers=4)
    for i_batch, sample_batch in enumerate(dataloader):
        print(i_batch, sample_batch["image"].size(), sample_batch["gender"])
        break
# 训练模型
import torch
from age_gender_dataset import AgeGenderDataset
from torch.utils.data import DataLoader


class MyMultipleTaskNet(torch.nn.Module):
    def __init__(self):
        super(MyMultipleTaskNet, self).__init__()
        self.cnn_layers = torch.nn.Sequential(
            # 卷积层(64x64x3的图像)
            torch.nn.Conv2d(3, 32, 3, padding=1),
            torch.nn.ReLU(),
            torch.nn.BatchNorm2d(32),
            torch.nn.MaxPool2d(2, 2),

            torch.nn.Conv2d(32, 64, 3, padding=1),
            torch.nn.ReLU(),
            torch.nn.BatchNorm2d(64),
            torch.nn.MaxPool2d(2, 2),

            # 32x32x32
            torch.nn.Conv2d(64, 96, 3, padding=1),
            torch.nn.ReLU(),
            torch.nn.BatchNorm2d(96),
            torch.nn.MaxPool2d(2, 2),

            torch.nn.Conv2d(96, 128, 3, padding=1),
            torch.nn.ReLU(),
            torch.nn.BatchNorm2d(128),
            torch.nn.MaxPool2d(2, 2),

            # 64x64x16
            torch.nn.Conv2d(128, 196, 3, padding=1),
            torch.nn.ReLU(),
            torch.nn.BatchNorm2d(196),
            torch.nn.MaxPool2d(2, 2),
        )
        # 全局最大池化
        self.global_max_pooling = torch.nn.AdaptiveMaxPool2d((1, 1))

        # 全连接层
        self.age_fc_layers = torch.nn.Sequential(
            torch.nn.Linear(196, 25),
            torch.nn.ReLU(),
            torch.nn.Linear(25, 1),
            torch.nn.Sigmoid()
        )

        self.gender_fc_layers = torch.nn.Sequential(
            torch.nn.Linear(196, 25),
            torch.nn.ReLU(),
            torch.nn.Linear(25, 2)
        )

    def forward(self, x):
        x = self.cnn_layers(x)

        # 2x2x196
        B, C, H, W = x.size()
        out = self.global_max_pooling(x).view(B, -1)
        out_age = self.age_fc_layers(out)
        out_gender = self.gender_fc_layers(out)
        return out_age, out_gender


if __name__ == '__main__':
    model = MyMultipleTaskNet()
    print(model)
    if torch.cuda.is_available():
        model.cuda()

    ds = AgeGenderDataset("D:/datasets/UTKFace")
    num_train_sample = ds.num_of_sample()
    bs = 16
    dataloader = DataLoader(ds, batch_size=bs, shuffle=True, num_workers=4)

    # 训练模型的次数
    num_epoch = 25
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
    model.train()

    # 损失函数
    mse_loss = torch.nn.MSELoss()
    cross_losss = torch.nn.CrossEntropyLoss()

    index = 0
    for epoch in range(num_epoch):
        train_loss = 0.0
        for i_batch, sample_batch in enumerate(dataloader):
            images_batch, age_batch, gender_batch = sample_batch["image"], sample_batch["age"], sample_batch["gender"]
            if torch.cuda.is_available():
                images_batch, age_batch, gender_batch = images_batch.cuda(), age_batch.cuda(), gender_batch.cuda()

            optimizer.zero_grad()
            m_age_out, m_gender_out = model(images_batch)

            age_batch = age_batch.view(-1, 1)
            gender_batch = gender_batch.long()

            loss = mse_loss(m_age_out, age_batch) + cross_losss(m_gender_out, gender_batch)
            loss.backward()

            optimizer.step()

            train_loss += loss.item()
            if index % 100 == 0:
                print("step {}\tTraining Loss {:.6f}".format(index, loss.item()))
            index += 1
        # 计算平均损失
        train_loss = train_loss / num_train_sample
        # 显示训练集与验证集的损失函数
        print("Epoch {}\tTraining Loss {:.6f}".format(epoch, train_loss))

    model.eval()
    torch.save(model, "D:/models/age_gender_model.pt")
    
# 验证
import torch
import numpy as np
import cv2 as cv
import time
from age_gender_cnn_test import MyMultipleTaskNet
from openvino.inference_engine import IECore

model_bin = "D:/netron_models/opencv_face_detector_uint8.pb"
config_text = "D:/netron_models/opencv_face_detector.pbtxt"
genders = ["male", "female"]


def video_age_gender_demo():
    cnn_model = torch.load("D:/models/age_gender_model.pt")
    cap = cv.VideoCapture("D:/images/01.mp4")

    net = cv.dnn.readNetFromTensorflow(model_bin, config=config_text)
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        h, w, c = frame.shape
        blobImage = cv.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0), False, False)
        net.setInput(blobImage)
        cvOut = net.forward()
        # 绘制检测矩形
        for detection in cvOut[0, 0, :, :]:
            score = float(detection[2])
            if score > 0.5:
                left = detection[3] * w
                top = detection[4] * h
                right = detection[5] * w
                bottom = detection[6] * h
                roi = frame[np.int32(top):np.int32(bottom), np.int32(left):np.int32(right), :]
                img = cv.resize(roi, (64, 64))
                img = (np.float32(img) / 255.0 - 0.5) / 0.5
                img = img.transpose((2, 0, 1))
                x_input = torch.from_numpy(img).view(1, 3, 64, 64)
                age, gender = cnn_model(x_input.cuda())
                predict_gender = torch.max(gender, 1)[1].cpu().detach().numpy()[0]
                gender = "Male"
                if predict_gender == 1:
                    gender = "Female"
                predict_age = age.cpu().detach().numpy()*116.0
                print(predict_gender, predict_age)
                # 绘制
                cv.putText(frame, ("gender: %s, age: %s") % (gender, int(predict_age[0][0])), (int(left), int(top)-15), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 1)
                cv.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (255, 0, 0), thickness=2)
                c = cv.waitKey(5)
        if c == ord("q"):
            break
        cv.imshow("face detection + landmark", frame)
    cv.waitKey(0)
    cv.destroyAllWindows()


def face_age_gender_demo():
    ie = IECore()
    for device in ie.available_devices:
        print(device)
    model_xml = "D:/netron_models/face-detection-0202.xml"
    model_bin = "D:/netron_models/face-detection-0202.bin"
    net = ie.read_network(model=model_xml, weights=model_bin)
    input_blob = next(iter(net.input_info))
    out_blob = next(iter(net.outputs))
    n, c, h, w = net.input_info[input_blob].input_data.shape
    print(n, c, h, w)
    cap = cv.VideoCapture("D:/images/01.mp4")
    exec_net = ie.load_network(network=net, device_name="CPU")

    # 加载性别与年龄预测模型
    em_net = ie.read_network(model="D:/netron_models/age_gender_model.onnx")
    em_input_blob = next(iter(em_net.input_info))
    em_it = iter(em_net.outputs)
    em_out_blob1 = next(em_it)
    em_out_blob2 = next(em_it)
    en, ec, eh, ew = em_net.input_info[em_input_blob].input_data.shape
    print(en, ec, eh, ew)

    em_exec_net = ie.load_network(network=em_net, device_name="CPU")
    while True:
        ret, frame = cap.read()
        if ret is not True:
            break
        image = cv.resize(frame, (w, h))
        image = image.transpose(2, 0, 1)
        inf_start = time.time()
        res = exec_net.infer(inputs={input_blob: [image]})
        inf_end = time.time() - inf_start
        # print("infer time(ms)：%.3f"%(inf_end*1000))
        ih, iw, ic = frame.shape
        res = res[out_blob]
        for obj in res[0][0]:
            if obj[2] > 0.75:
                xmin = int(obj[3] * iw)
                ymin = int(obj[4] * ih)
                xmax = int(obj[5] * iw)
                ymax = int(obj[6] * ih)
                if xmin < 0:
                    xmin = 0
                if ymin < 0:
                    ymin = 0
                if xmax >= iw:
                    xmax = iw - 1
                if ymax >= ih:
                    ymax = ih - 1
                roi = frame[ymin:ymax, xmin:xmax, :]
                roi_img = cv.resize(roi, (ew, eh))
                roi_img = (np.float32(roi_img) / 255.0 - 0.5) / 0.5
                roi_img = roi_img.transpose(2, 0, 1)
                em_res = em_exec_net.infer(inputs={em_input_blob: [roi_img]})
                gender_prob = em_res[em_out_blob1].reshape(1, 2)
                prob_age = em_res[em_out_blob2].reshape(1, 1)[0][0] * 116
                label_index = np.int(np.argmax(gender_prob, 1))
                age = np.int(prob_age)
                cv.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 255), 2, 8)
                cv.putText(frame, "infer time(ms): %.3f" % (inf_end * 1000), (50, 50), cv.FONT_HERSHEY_SIMPLEX, 1.0,
                           (255, 0, 255),
                           2, 8)
                cv.putText(frame, genders[label_index] + ', ' + str(age), (xmin, ymin), cv.FONT_HERSHEY_SIMPLEX, 1.0,
                           (0, 0, 255),
                           2, 8)
        cv.imshow("Face+ age/gender prediction", frame)
        c = cv.waitKey(1)
        if c == 27:
            break
    cv.waitKey(0)
    cv.destroyAllWindows()


if __name__ == '__main__':
    # video_age_gender_demo()
    face_age_gender_demo()
```

### 9.实战多标签实现验证码识别

```python
'''
one-hot编码
残差网络结构
	改进卷积神经网络深度退化问题
'''
import os

import cv2
import numpy as np
import torch
import os
from torchvision import transforms
from torch.utils.data import DataLoader
from torch.utils.data import Dataset

NUMBER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
ALPHABET = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
ALL_CHAR_SET = NUMBER + ALPHABET
ALL_CHAR_SET_LEN = len(ALL_CHAR_SET)
MAX_CAPTCHA = 5


def output_nums():
    return MAX_CAPTCHA * ALL_CHAR_SET_LEN


def encode(a):
    onehot = [0] * ALL_CHAR_SET_LEN
    idx = ALL_CHAR_SET.index(a)
    onehot[idx] = 1
    return onehot


class CapchaDataset(Dataset):
    def __init__(self, root_dir):
        self.transform = transforms.Compose([transforms.ToTensor(),
                                             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                                             transforms.Resize((32, 128))])
        img_files = os.listdir(root_dir)
        self.txt_labels = []
        self.encodes = []
        self.images = []
        for file_name in img_files:
            label = file_name[:-4]
            label_oh = []
            for i in label:
                label_oh += encode(i)
            self.images.append(os.path.join(root_dir, file_name))
            self.encodes.append(np.array(label_oh))
            self.txt_labels.append(label)

    def __len__(self):
        return len(self.images)

    def num_of_samples(self):
        return len(self.images)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            image_path = self.images[idx]
        else:
            image_path = self.images[idx]
        img = cv2.imread(image_path)
        img = self.transform(img)
        h, w, c = img.shape
        # resize宽高
        # img = cv2.resize(img, (128, 32))
        # img = (np.float32(img) / 255.0 - 0.5) / 0.5
        # img = img.transpose((2, 0, 1))
        # img = torch.permute(img, (2, 0, 1))
        # sample = {"image": torch.from_numpy(img), "encode": self.encodes[idx], "label": self.txt_labels[idx]}
        sample = {"image": img, "encode": self.encodes[idx], "label": self.txt_labels[idx]}
        return sample


if __name__ == '__main__':
    ds = CapchaDataset("D:/datasets/Verification_Code/samples")
    for i in range(len(ds)):
        sample = ds[i]
        print(i, sample["image"].size(), sample["label"], sample["encode"].shape)
        print(sample["encode"].reshape(5, -1))
        if i == 3:
            break

    dataloader = DataLoader(ds, batch_size=4, shuffle=True, num_workers=4)
    for i_batch, sample_batch in enumerate(dataloader):
        print(i_batch, sample_batch["image"].size(), sample_batch["label"])
        break

import torch

from torch import nn
from torch.utils.data import DataLoader
from verification_code_dataset import output_nums
from verification_code_dataset import CapchaDataset


class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.skip = nn.Sequential()

        # 步长为2的话会下采样，也就是降采样
        if stride != 1 or in_channels != out_channels:
            self.skip = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

        self.block = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )

    def forward(self, x):
        out = self.block(x)
        identity = self.skip(x)
        out += identity
        out = nn.functional.relu(out)
        return out


class CapchaResNet(nn.Module):
    def __init__(self):
        super(CapchaResNet, self).__init__()
        self.cnn_layers = nn.Sequential(
            # 卷积层 128*32*3
            ResidualBlock(3, 32, 1),
            ResidualBlock(32, 64, 2),
            ResidualBlock(64, 64, 2),
            ResidualBlock(64, 128, 2),
            ResidualBlock(128, 256, 2),
            ResidualBlock(256, 256, 2)
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(256 * 4, output_nums())
        )

    def forward(self, x):
        x = self.cnn_layers(x)
        out = x.view(-1, 4 * 256)
        out = self.fc_layers(out)
        return out


if __name__ == '__main__':
    model = CapchaResNet()
    print(model)
    gpu_on = torch.cuda.is_available()
    if gpu_on:
        model.cuda()
    ds = CapchaDataset("D:/datasets/Verification_Code/samples")
    num_train_samples = ds.num_of_samples()
    bs = 16

    dataloader = DataLoader(ds, batch_size=bs, shuffle=True)

    # 训练模型次数
    num_epochs = 25
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
    model.train()

    # 损失函数 one-hot编码对应的损失函数
    mul_loss = torch.nn.MultiLabelSoftMarginLoss()
    index = 0
    for epoch in range(num_epochs):
        train_loss = 0.0
        for i_batch, sample_batch in enumerate(dataloader):
            image_batch, oh_labels = sample_batch['image'], sample_batch['encode']
            if gpu_on:
                image_batch, oh_labels = image_batch.cuda(), oh_labels.cuda()

            optimizer.zero_grad()
            m_label_out = model(image_batch)
            oh_labels = torch.autograd.Variable(oh_labels.float())

            loss = mul_loss(m_label_out, oh_labels)

            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            if index % 100 == 0:
                print("step: {}\tTraining Loss: {:.6f}".format(index, loss.item()))
            index += 1
        train_loss = train_loss / num_train_samples
        print("Epoch: {}\tTraining Loss: {:.6f}".format(epoch, train_loss))
    model.eval()
    torch.save(model, "D:/models/verification_code_model.pt")

import os
import torch
import cv2 as cv
import numpy as np
from verification_code_model import ResidualBlock, CapchaResNet

NUMBER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
ALPHABET = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
ALL_CHAR_SET = NUMBER + ALPHABET
ALL_CHAR_SET_LEN = len(ALL_CHAR_SET)
MAX_CAPTCHA = 5


def test_capcha_codes():
    cnn_model = torch.load("D:/models/verification_code_model.pt")
    root_dir = "D:/datasets/Verification_Code/testdata"
    files = os.listdir(root_dir)
    one_hot_len = ALL_CHAR_SET_LEN
    for file in files:
        if os.path.isfile(os.path.join(root_dir, file)):
            image = cv.imread(os.path.join(root_dir, file))
            img = cv.resize(image, (128, 32))
            img = (np.float32(img) / 255.0 - 0.5) / 0.5
            img = img.transpose((2, 0, 1))
            x_input = torch.from_numpy(img).view(1, 3, 32, 128)
            probs = cnn_model(x_input.cuda())
            mul_pred_labels = probs.squeeze().cpu().tolist()
            c0 = ALL_CHAR_SET[np.argmax(mul_pred_labels[0:one_hot_len])]
            c1 = ALL_CHAR_SET[np.argmax(mul_pred_labels[one_hot_len:one_hot_len*2])]
            c2 = ALL_CHAR_SET[np.argmax(mul_pred_labels[one_hot_len*2:one_hot_len*3])]
            c3 = ALL_CHAR_SET[np.argmax(mul_pred_labels[one_hot_len*3:one_hot_len*4])]
            c4 = ALL_CHAR_SET[np.argmax(mul_pred_labels[one_hot_len*4:one_hot_len*5])]
            pred_txt = "%s%s%s%s%s" %(c0, c1, c2, c3, c4)
            cv.putText(image, pred_txt, (10, 20), cv.FONT_HERSHEY_PLAIN, 1.5, (0, 100, 150), 2)
            print("current code : %s , predict code : %s" % (file[:-4], pred_txt))
            cv.imshow("capcha predict", image)
            cv.waitKey(0)


if __name__ == '__main__':
    test_capcha_codes()
```

### 10.实战-自定义残差网络车辆识别

```python
'''
车辆属性数据集
6 vehicle types，9850images
http://iitlab.bit.edu.cn/mcislab/vehicledb/
颜色: ["white", "gray", "yellow", "red", "green", "blue", "truck", "van"]
类别: ["car", "bus", "truck", "van"]
残差block结构
全局最大池化
多分支任务预测(颜色与类别)
'''
import os
import torch
import numpy as np
from torch import nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torchvision import transforms
import cv2 as cv

colors = ["white", "gray", "yellow", "red", "green", "blue", "black", "van"]
types = ["car", "bus", "truck", "van"]


class VehicleAttrsDataset(Dataset):
    def __init__(self, root_dir):
        super(VehicleAttrsDataset, self).__init__()
        self.transform = transforms.Compose([transforms.ToTensor(),
                                             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
                                             transforms.Resize((72, 72))])
        self.vehicle_colors = []
        self.images = []
        self.vehicle_types = []
        index = 0
        for file_name in os.listdir(root_dir):
            color, vehicle_type = file_name.split("_")[:2]
            self.vehicle_types.append(np.float32(types.index(vehicle_type)))
            self.vehicle_colors.append(np.float32(colors.index(color)))
            path = os.path.join(root_dir, file_name)
            self.images.append(path)
            index += 1

    def __len__(self):
        return len(self.images)

    def nums_of_sample(self):
        return len(self.images)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            image_path = self.images[idx]
        else:
            image_path = self.images[idx]
        img = cv.imread(image_path)
        img = self.transform(img)
        sample = {"image": img, "color": self.vehicle_colors[idx], "type": self.vehicle_types[idx]}
        return sample


if __name__ == '__main__':
    ds = VehicleAttrsDataset("D:/datasets/vehicle_attrs_dataset")
    for i in range(len(ds)):
        sample = ds[i]
        print(i, sample["image"].size(), sample["color"])
        if i == 3:
            break
    dataloader = DataLoader(ds, batch_size=4, shuffle=True, num_workers=4)
    for i_batch, sample_batch in enumerate(dataloader):
        print(i_batch, sample_batch["image"].size(), sample_batch["type"])
        break

import torch
import numpy as np
from torch import nn
from vehicle_attrs_dataset import VehicleAttrsDataset
from torch.utils.data import DataLoader


class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.skip = nn.Sequential()

        # 步长为2的话会下采样，也就是降采样
        if stride != 1 or in_channels != out_channels:
            self.skip = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

        self.block = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )

    def forward(self, x):
        out = self.block(x)
        identity = self.skip(x)
        out += identity
        out = nn.functional.relu(out)
        return out


class VehicleAttributeResNet(nn.Module):
    def __init__(self):
        super(VehicleAttributeResNet, self).__init__()
        self.cnn_layers = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.MaxPool2d(2, 2),

            ResidualBlock(32, 64),
            nn.MaxPool2d(2, 2),

            ResidualBlock(64, 128),
            nn.MaxPool2d(2, 2)
        )
        # 全局最大池化
        self.global_max_pooling = nn.AdaptiveMaxPool2d((1, 1))
        self.color_fc_layers = nn.Sequential(
            nn.Linear(128, 7),
            nn.Sigmoid()
        )
        self.type_fc_layers = nn.Sequential(
            nn.Linear(128, 4)
        )

    def forward(self, x):

        x = self.cnn_layers(x)
        B, C, H, W = x.size()
        out = self.global_max_pooling(x).view(B, -1)
        out_color = self.color_fc_layers(out)
        out_type = self.type_fc_layers(out)
        return out_color, out_type


if __name__ == '__main__':
    model = VehicleAttributeResNet()
    gpu_on = torch.cuda.is_available()
    if gpu_on:
        model.cuda()
    ds = VehicleAttrsDataset("D:/datasets/vehicle_attrs_dataset")
    num_train_samples = ds.nums_of_sample()
    dataloader = DataLoader(ds, batch_size=16, shuffle=True, num_workers=4)

    num_epochs = 20
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)

    model.train()
    cross_loss = torch.nn.CrossEntropyLoss()
    index = 0
    for epoch in range(num_epochs):
        train_loss = 0.0
        for i_batch, sample_batch in enumerate(dataloader):
            image_batch, color_batch, type_batch = sample_batch["image"], sample_batch["color"], sample_batch["type"]
            if gpu_on:
                image_batch = image_batch.cuda()
                color_batch = color_batch.cuda()
                type_batch = type_batch.cuda()
            optimizer.zero_grad()

            color_out, type_out = model(image_batch)
            color_batch = color_batch.long()
            type_batch = type_batch.long()

            loss = cross_loss(color_out, color_batch) + cross_loss(type_out, type_batch)

            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            if index % 100 == 0:
                print("step {}\t Training Loss {:.6f}".format(index, loss.item()))
            index += 1
    model.eval()
    torch.save(model, "D:/models/vehicle_attrs_model.pt")

import cv2 as cv
import numpy as np
import torch
from torch import nn
from vehicle_attrs_model import ResidualBlock, VehicleAttributeResNet
from openvino.inference_engine import IECore


color_labels = ["white", "gray", "yellow", "red", "green", "blue", "black"]
type_labels = ["car", "bus", "truck", "van"]

model_dir = "D:/models/"
model_xml = model_dir + "vehicle-detection-adas-0002.xml"
model_bin = model_dir + "vehicle-detection-adas-0002.bin"

ie = IECore()
net = ie.read_network(model=model_xml, weights=model_bin)

versions = ie.get_versions("CPU")
cnn_model = torch.load("D:/models/vehicle_attrs_model.pt")
input_blob = next(iter(net.input_info))
n, c, h, w = net.input_info[input_blob].input_data.shape

capture = cv.VideoCapture("D:/videos/cars_1900.mp4")
ih = capture.get(cv.CAP_PROP_FRAME_HEIGHT)
iw = capture.get(cv.CAP_PROP_FRAME_WIDTH)

input_blob = next(iter(net.input_info))
out_blob = next(iter(net.outputs))

exec_net = ie.load_network(network=net, device_name="CPU")
while True:
    ret, src = capture.read()
    if not ret:
        break
    images = np.ndarray(shape=(n, c, h, w))
    images_hw = []
    sh, sw = src.shape[:-1]
    images_hw.append((sh, sw))
    if (sh, sw) != (h, w):
        image = cv.resize(src, (w, h))
    image = image.transpose((2, 0, 1))
    images[0] = image
    res = exec_net.infer(inputs={input_blob: images})

    # 解析ssd输出内容
    res = res[out_blob]
    license_score = []
    license_boxs = []
    index = 0
    data = res[0][0]
    for number, proposal in enumerate(data):
        if proposal[2] > 0.75:
            sh, sw = images_hw[0]
            label = np.int(proposal[1])
            confidence = proposal[2]
            xmin = np.int(sw * proposal[3])
            ymin = np.int(sh * proposal[4])
            xmax = np.int(sw * proposal[5])
            ymax = np.int(sh * proposal[6])

            if xmin < 0:
                xmin = 0
            if ymin < 0:
                ymin = 0
            if xmax >= sw:
                xmax = sw - 1
            if ymax >= sh:
                ymax = sh - 1
            vehicle_roi = src[ymin:ymax, xmin:xmax]
            img = cv.resize(vehicle_roi, (72, 72))
            img = (np.float32(img) / 255.0 - 0.5) / 0.5
            img = img.transpose((2, 0, 1))
            x_input = torch.from_numpy(img).view(1, 3, 72, 72)
            color_, type_ = cnn_model(x_input.cuda())
            predict_color = torch.max(color_, 1)[1].cpu().detach().numpy()[0]
            predict_type = torch.max(type_, 1)[1].cpu().detach().numpy()[0]
            attrs_txt = "color:%s, type:%s" % (color_labels[predict_color], type_labels[predict_type])
            cv.rectangle(src, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)
            cv.putText(src, attrs_txt, (xmin, ymin), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)
    cv.imshow("Vehicle Attributes Recognition Demo", src)
    res_key = cv.waitKey(1)
    if res_key == ord("q"):
        break
```

### 11.实战全卷积网络表情识别

```python
'''
全卷积网络FCN
	深度神经网络-卷积神经网络CNN+FC
	CNN表示卷积层
	FC表示全链接层
残差block + 输出头N*8*1*1
分类损失-交叉熵损失
'''
# dataset
import torch
import numpy as np
import cv2 as cv
import os
from torch import nn
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torchvision import transforms


emotion_labels = ["neutral","anger","disdain","disgust","fear","happy","sadness","surprise"]


class EmotionsDataset(Dataset):
    def __init__(self, root_dir):
        super(EmotionsDataset, self).__init__()
        self.transform = transforms.Compose([transforms.ToTensor(),
                                             transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
                                             transforms.Resize((64, 64))])
        self.emotions = []
        self.images = []
        index = 0
        for file_name in os.listdir(root_dir):
            emotion = file_name.split("_")[0]
            self.emotions.append(np.float32(np.int32(emotion)))
            self.images.append(os.path.join(root_dir, file_name))
            index += 1

    def __len__(self):
        return len(self.images)

    def num_of_sample(self):
        return len(self.images)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        image_path = self.images[idx]
        img = cv.imread(image_path)
        sample = {"image": self.transform(img), "emotion": self.emotions[idx]}
        return sample


if __name__ == '__main__':
    ds = EmotionsDataset("D:/datasets/emotion_dataset")
    for i in range(len(ds)):
        sample = ds[i]
        print(i, sample["image"].size(), sample["emotion"])
        if i == 3:
            break
    dataloader = DataLoader(ds, batch_size=4, shuffle=True, num_workers=4)
    for i_batch, sample_batch in enumerate(dataloader):
        print(i_batch, sample_batch["image"].size(), sample_batch["emotion"])
        break
        
# train model
import torch
import numpy as np
from torch import nn
from torch.utils.data import DataLoader
from torchvision import transforms
from emotions_dataset import EmotionsDataset


train_gpu = torch.cuda.is_available()
num_iden = 0


class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()

        self.skip = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.skip = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        self.block = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )

    def forward(self, x):
        out = self.block(x)
        identity = self.skip(x)
        if identity.any():
            global num_iden
            num_iden += 1
        out += identity
        out = nn.functional.relu(out)
        return out


class EmotionResNet(nn.Module):
    def __init__(self):
        super(EmotionResNet, self).__init__()
        self.cnn_layers = nn.Sequential(
            ResidualBlock(3, 32, 1),
            ResidualBlock(32, 64, 2),
            ResidualBlock(64, 64, 2),
            ResidualBlock(64, 128, 2),
            ResidualBlock(128, 128, 2),
            ResidualBlock(128, 256, 2),
            ResidualBlock(256, 256, 2),
            ResidualBlock(256, 8, 1),
        )

    def forward(self, x):
        x = self.cnn_layers(x)
        B, C, H, W = x.shape
        out = x.view(B, -1)
        return out


if __name__ == '__main__':
    model = EmotionResNet()
    if train_gpu:
        model.cuda()

    ds = EmotionsDataset("D:/datasets/emotion_dataset")
    num_train_samples = ds.num_of_sample()
    dataloader = DataLoader(ds, batch_size=8, shuffle=True, num_workers=4)
    num_epochs = 20
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
    model.train()

    cross_loss = nn.CrossEntropyLoss()
    index = 0
    for epoch in range(num_epochs):
        train_loss = 0.0
        for i_batch, sample_batch in enumerate(dataloader):
            image_batch, emotion_batch = sample_batch["image"], sample_batch["emotion"]
            if train_gpu:
                image_batch = image_batch.cuda()
                emotion_batch = emotion_batch.cuda()
            emotion_out = model(image_batch)
            emotion_batch = emotion_batch.long()
            loss = cross_loss(emotion_out, emotion_batch)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            if index % 100 == 0:
                print("step: {}\tTraining Loss {:.6f}".format(index, loss.item()))
            index += 1
        train_loss = train_loss / num_train_samples
        print("Epoch {}\tTraining Loss {:.6f}".format(epoch, train_loss))
    model.eval()
    torch.save(model, "D:/models/emotion_model.pt")
    print(num_iden)
    
# openvino detection
import cv2 as cv
import numpy as np
import time
from openvino.inference_engine import IECore

emotion_labels = ["neutral","anger","disdain","disgust","fear","happy","sadness","surprise"]


def face_emotion_demo():
    ie = IECore()
    for device in ie.available_devices:
        print(device)
    model_xml = "D:/models/face-detection-0202.xml"
    model_bin = "D:/models/face-detection-0202.bin"

    net = ie.read_network(model=model_xml, weights=model_bin)
    input_blob = next(iter(net.input_info))
    out_blob = next(iter(net.outputs))

    n, c, h, w = net.input_info[input_blob].input_data.shape
    print(n, c, h, w)

    cap = cv.VideoCapture("D:/videos/Boogie_UP.mp4")
    exec_net = ie.load_network(network=net, device_name="CPU")
    # 加载性别与年龄预测模型
    em_net = ie.read_network(model="D:/models/face_emotions_model.onnx")
    em_input_blob = next(iter(em_net.input_info))
    em_it = iter(em_net.outputs)
    em_out_blob1 = next(em_it)
    en, ec, eh, ew = em_net.input_info[em_input_blob].input_data.shape
    print(en, ec, eh, ew)

    em_exec_net = ie.load_network(network=em_net, device_name="CPU")

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        image = cv.resize(frame, (w, h))
        image = image.transpose(2, 0, 1)
        inf_start = time.time()
        res = exec_net.infer(inputs={input_blob: [image]})
        inf_end = time.time() - inf_start
        # print("infer time(ms)：%.3f"%(inf_end*1000))
        ih, iw, ic = frame.shape
        res = res[out_blob]
        for obj in res[0][0]:
            if obj[2] > 0.75:
                xmin = int(obj[3] * iw)
                ymin = int(obj[4] * ih)
                xmax = int(obj[5] * iw)
                ymax = int(obj[6] * ih)
                if xmin < 0:
                    xmin = 0
                if ymin < 0:
                    ymin = 0
                if xmax >= iw:
                    xmax = iw - 1
                if ymax >= ih:
                    ymax = ih - 1
                roi = frame[ymin:ymax, xmin:xmax, :]
                roi_img = cv.resize(roi, (ew, eh))
                roi_img = (np.float32(roi_img) / 255.0 - 0.5) / 0.5
                roi_img = roi_img.transpose(2, 0, 1)
                em_res = em_exec_net.infer(inputs={em_input_blob: [roi_img]})
                emotion_prob = em_res[em_out_blob1]  # 1x8
                label_index = np.int(np.argmax(emotion_prob, 1))
                cv.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 255), 2, 8)
                cv.putText(frame, "infer time(ms): %.3f" % (inf_end * 1000), (50, 50), cv.FONT_HERSHEY_SIMPLEX, 1.0,
                           (255, 0, 255),
                           2, 8)
                cv.putText(frame, emotion_labels[label_index], (xmin, ymin), cv.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255),
                           2, 8)
        cv.imshow("Face Emotions Detection Demo", frame)
        c = cv.waitKey(1)
        if c == 27:
            break
    cv.waitKey(0)
    cv.destroyAllWindows()


if __name__ == '__main__':
    face_emotion_demo()
```

### 11.迁移学习

```python
'''
torchvision 
图像分类：VGG、ResNet、MobileNet、Inception
图像检测：Faster-RCNN、Mask-RCNN、RetinaNet
图像分割：FNC、DeepLab、LR-ASPP
torchvision.transforms
支持PIL与tensor(RGB)  opencv(BGR)
'''

```

### 12.实战-ResNet18基于图像分类工业缺陷检测

```python
'''
ResNet18残差网络
import torchvision.models as models
'''
# dataset
import torch
import numpy as np
import os
import cv2 as cv
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torchvision import transforms

# 夹杂 - In - inclusion
# 划痕 - SC - scratch
# 裂纹 - CR- crackle
# 压入氧化皮 - PS - Press in oxide scale
# 麻点 - RS
# 斑点 - PA
defect_labels = ["In", "Sc", "Cr", "PS", "RS", "Pa"]


class SurfaceDefectDataset(Dataset):
    def __init__(self, root_dir):
        self.transform = transforms.Compose([transforms.ToTensor(),
                                             transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                                  std=[0.229, 0.224, 0.225]),
                                             transforms.Resize((200, 200))])

        img_file = os.listdir(root_dir)
        self.defect_types = []
        self.images = []
        index = 0
        for file_name in img_file:
            defect_attrs = file_name.split("_")
            d_index = defect_labels.index(defect_attrs[0])
            self.images.append(os.path.join(root_dir, file_name))
            self.defect_types.append(d_index)
            index += 1

    def __len__(self):
        return len(self.images)

    def num_of_samples(self):
        return len(self.images)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        img_path = self.images[idx]
        img = cv.imread(img_path)
        img = cv.cvtColor(img, cv.COLOR_BGR2RGB)

        sample = {"image": self.transform(img), "defect": self.defect_types[idx]}
        return sample


if __name__ == '__main__':
    ds = SurfaceDefectDataset("D:/datasets/enu_surface_defect/train")
    for i in range(len(ds)):
        sample = ds[i]
        print(i, sample["image"].size(), sample["defect"])
        if i == 3:
            break

    dataloader = DataLoader(ds, batch_size=4, shuffle=True, num_workers=4)
    for i_batch, sample_batch in enumerate(dataloader):
        print(i_batch, sample_batch["image"].size(), sample_batch["defect"])
        break

# train model
import torch
from torch.utils.data import DataLoader
from torchvision import models
from torch import nn
from surface_detect_dataset import SurfaceDefectDataset

train_gpu = torch.cuda.is_available()


class SurfaceDefectResNet(nn.Module):
    def __init__(self):
        super(SurfaceDefectResNet, self).__init__()
        # pretrained=True下载模型
        self.cnn_layers = models.resnet18(pretrained=False)
        # 手动导入模型
        self.cnn_layers.load_state_dict(torch.load("D:/models/resnet18-f37072fd.pth"))
        num_ftrs = self.cnn_layers.fc.in_features
        self.cnn_layers.fc = torch.nn.Linear(num_ftrs, 6)

    def forward(self, x):
        out = self.cnn_layers(x)
        return out


def demo():
    model = SurfaceDefectResNet()
    print(model)

    if train_gpu:
        model.cuda()

    ds = SurfaceDefectDataset("D:/datasets/enu_surface_defect/train")
    num__train_samples = ds.num_of_samples()
    dataloader = DataLoader(ds, batch_size=32, shuffle=True, num_workers=4)

    num_epochs = 20
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
    model.train()

    cross_loss = torch.nn.CrossEntropyLoss()
    index = 0
    for epoch in range(num_epochs):
        train_loss = 0.0
        for i_batch, sample_batch in enumerate(dataloader):
            image_batch, label_batch = sample_batch["image"], sample_batch["defect"]
            if train_gpu:
                image_batch = image_batch.cuda()
                label_batch = label_batch.cuda()
            optimizer.zero_grad()

            label_out = model(image_batch)
            label_batch = label_batch.long()

            loss = cross_loss(label_out, label_batch)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            if index % 100 == 0:
                print("step {}, Training Loss {:.6f}".format(index, loss.item()))
            index += 1
        train_loss = train_loss / num__train_samples

        print("Epoch {}, Training Loss {:.6f}".format(epoch, train_loss))
    model.eval()
    torch.save(model.state_dict(), "D:/models/surface_defect_model.pt")

# demo
import os
import cv2 as cv
import torch
import numpy as np
from torchvision import transforms
from surface_detect_model import SurfaceDefectResNet

defect_labels = ["In", "Sc", "Cr", "PS", "RS", "Pa"]


def defect_demo():
    cnn_model = SurfaceDefectResNet()
    cnn_model.load_state_dict(torch.load("D:/models/surface_defect_model.pt"))
    cnn_model.eval()
    cnn_model.cuda()
    print(cnn_model)
    root_dir = "D:/datasets/enu_surface_defect/test"
    img_transform = transforms.Compose([transforms.ToTensor(),
                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                             std=[0.229, 0.224, 0.225]),
                                        transforms.Resize((200, 200))])
    filenames = os.listdir(root_dir)
    for file in filenames:
        image = cv.imread(os.path.join(root_dir, file))
        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)
        x_input = img_transform(image).view(1, 3, 200, 200)
        probs = cnn_model(x_input.cuda())
        predic_ = probs.view(6).cpu().detach().numpy()
        idx = np.argmax(predic_)
        defect_txt = defect_labels[idx]
        print(defect_txt, file)
        cv.putText(image, defect_txt, (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1.0, (30, 120, 200), 2, 8)
        cv.imshow("defect_detection", image)
        cv.waitKey(0)
    cv.destroyAllWindows()


if __name__ == '__main__':
    defect_demo()
```

### 13.Faster-RCNN自定义对象检测

```python
'''
torchvision对象检测
分类/类别+位置
位置损失
类别损失
COCO数据集，支持80个类别、MS-COCO数据集
Faster-RCNN结构
	特征提取-backbone：ResNet、VGG、Inception系列
	区域推荐-RPN: 位置信息+对象预测，IOU交并比
	RPN损失方式：类别损失与位置损失，多任务损失、类别损失、位置损失
	图像金字塔(上采样)、卷积Feature Map、多尺度、FPN特征金字塔 : 提高检测精度
	ROI Pooling对齐
	分类预测
	Bounding-box回归
	对象预测
计算mAP
pip install git+https://github.com/cocodataset/cocoapi/tree/master/PythonAPI
学习率策略：指数衰减、多步长衰减、指定步长
torch.optim.lr_scheduler.StepLR()
'''
# dataset
import os
import cv2
import torch
import numpy as np
from torch import nn
from faster_rcnn_transform import Compose
from faster_rcnn_transform import ToTensor
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from xml.dom.minidom import parse


class PetDataset(Dataset):
    def __init__(self, root_dir):
        super(PetDataset, self).__init__()
        self.root_dir = root_dir
        self.transform = Compose([ToTensor()])
        self.ann_xmls = list(sorted(os.listdir(os.path.join(root_dir, "annotations/xmls"))))

    def __len__(self):
        return len(self.ann_xmls)

    def num_of_samples(self):
        return len(self.ann_xmls)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        xml_path = os.path.join(self.root_dir, "annotations/xmls", self.ann_xmls[idx])

        dom = parse(xml_path)
        data = dom.documentElement
        objects = data.getElementsByTagName("object")
        node = data.getElementsByTagName("filename")[0]
        file_name = node.childNodes[0].nodeValue
        image_path = os.path.join(self.root_dir, "images", file_name)
        img = cv2.imread(image_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        boxes = []
        labels = []
        for obj in objects:
            name = obj.getElementsByTagName("name")[0].childNodes[0].nodeValue
            if name == "dog":
                labels.append(np.int(1))
            if name == "cat":
                labels.append(np.int(2))
            bndbox = obj.getElementsByTagName("bndbox")[0]
            xmin = np.float(bndbox.getElementsByTagName("xmin")[0].childNodes[0].nodeValue)
            ymin = np.float(bndbox.getElementsByTagName("ymin")[0].childNodes[0].nodeValue)
            xmax = np.float(bndbox.getElementsByTagName("xmax")[0].childNodes[0].nodeValue)
            ymax = np.float(bndbox.getElementsByTagName("ymax")[0].childNodes[0].nodeValue)
            boxes.append([xmin, ymin, xmax, ymax])
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        image_id = torch.tensor([idx])
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        iscrowd = torch.zeros((len(objects,), ), dtype=torch.int64)
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd
        img, target = self.transform(img, target)
        return img, target


if __name__ == '__main__':
    ds = PetDataset("D:/datasets/pet_data")
    for i in range(len(ds)):
        img, target = ds[i]
        print(i, img.size(), target)
        device = torch.device("cuda:0")
        boxes = target["boxes"]
        xmin, ymin, xmax, ymax = boxes.unbind(1)
        targets = [{k: v.to(device) for k, v in t.items()} for t in [target]]
        if i == 3:
            break

# train
import torch
from torchvision import models
from pytorch_vision_detection.engine import train_one_epoch
from faster_rcnn_dataset import PetDataset
from pytorch_vision_detection import utils
from torch.utils.data import DataLoader


def train_main():
    train_gpu = torch.cuda.is_available()

    # progress=True, num_classes=num_classes, pretrained_backbone=True
    num_classes = 3
    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=num_classes, pretrained_backbone=True)
    device = torch.device("cuda:0")
    model.to(device)
    dataset = PetDataset("D:/datasets/pet_data")
    data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)

    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

    num_epoch = 20
    for epoch in range(num_epoch):
        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)
        lr_scheduler.step()
    torch.save(model.state_dict(), "D:/models/faster_rcnn_model.pt")


if __name__ == '__main__':
    train_main()

# demo
import torchvision
import torch
import cv2 as cv
import numpy as np

num_classes = 3
coco_names = {'0': 'unknown', '1': 'dog', '2': 'cat'}

model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=num_classes,
                                                             pretrained_backbone=True)
model.load_state_dict(torch.load("D:/models/faster_rcnn_model.pt"))
model.eval()
transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

train_gpu = torch.cuda.is_available()
if train_gpu:
    model.cuda()


def pet_image_detection():
    image = cv.imread("D:/images/gou.jpg")
    image = cv.resize(image, (720, 480))
    cv.imshow("input", image)
    blob = transform(image)
    c, h, w = blob.shape
    input_x = blob.view(1, c, h, w)
    output = model(input_x.cuda())[0]
    boxes = output["boxes"].cpu().detach().numpy()
    scores = output["scores"].cpu().detach().numpy()
    labels = output["labels"].cpu().detach().numpy()
    print(boxes.shape, scores.shape, labels.shape)
    index = 0
    for x1, y1, x2, y2 in boxes:
        if scores[index] > 0.5:
            cv.rectangle(image, (np.int32(x1), np.int32(y1)), (np.int32(x2), np.int32(y2)), (140, 199, 0), 2, 8)
            label_id = labels[index]
            label_txt = coco_names[str(label_id)]
            cv.putText(image, label_txt, (np.int32(x2), np.int32(y2)), cv.FONT_HERSHEY_SIMPLEX, 1.0, (60, 120, 240), 2)
            index += 1
    cv.imshow("faster-rcnn Pet detection", image)
    cv.waitKey(0)
    cv.destroyAllWindows()


if __name__ == '__main__':
    pet_image_detection()
```

### 14.Mask-RCNN行人实例分割训练与检测

```python
'''
论文网址
https://arxiv.org
Mask-RCNN = Faster-RCNN + FCN
Backbone + FPN(可选) + RPN + ROI Align
ResNet50/101 + Feature Maps + RPN + Roi Align 
ROI Align处理
	ROI对齐
	ROI Pooling
	ROI Align
	像素精度与插值方式
Mask-RCNN 对象检测 + 实例分割
数据集：https://www.cis.upenn.edu/~jshi/ped_html
标注工具：PixelAnnotationTool
'''
# dataset
import os
import torch
import numpy as np
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from pytorch_vision_detection import transforms
from PIL import Image


class PennFudanDataset(Dataset):
    def __init__(self, root_dir):
        self.root_dir = root_dir
        self.transform = transforms.Compose([transforms.ToTensor()])
        self.imgs = list(sorted(os.listdir(os.path.join(root_dir, "PNGImages"))))
        self.masks = list(sorted(os.listdir(os.path.join(root_dir, "PedMasks"))))

    def __len__(self):
        return len(self.imgs)

    def num_of_samples(self):
        return len(self.imgs)

    def __getitem__(self, idx):
        img_path = os.path.join(self.root_dir, "PNGImages", self.imgs[idx])
        mask_path = os.path.join(self.root_dir, "PedMasks", self.masks[idx])
        img = Image.open(img_path).convert("RGB")
        mask = Image.open(mask_path)
        mask = np.array(mask)
        obj_ids = np.unique(mask)
        obj_ids = obj_ids[1:]
        masks = mask == obj_ids[:, None, None]
        num_objs = len(obj_ids)
        boxes = []
        for i in range(num_objs):
            pos = np.where(masks[i])
            xmin = np.min(pos[1])
            xmax = np.max(pos[1])
            ymin = np.min(pos[0])
            ymax = np.max(pos[0])
            boxes.append([xmin, ymin, xmax, ymax])

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.ones((num_objs,), dtype=torch.int64)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        image_id = torch.tensor([idx])
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        iscrowd = torch.zeros((num_objs, ), dtype=torch.int64)
        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["masks"] = masks
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transform is not None:
            img, target = self.transform(img, target)
        return img, target
if __name__ == '__main__':
    ds = PennFudanDataset("D:/datasets/PennFudanPed")
    for i in range(len(ds)):
        img, target = ds[i]
        print(i, img.size(), target)
        device = torch.device("cuda:0")
        boxes = target["boxes"]
        xmin, ymin, xmax, ymax = boxes.unbind(1)
        targets = [{k: v.to(device) for k, v in t.items()} for t in [target]]
        if i == 3:
            break
# train
import torch
from torchvision import models
from torch.utils.data import DataLoader

from pytorch_vision_detection import utils
from pytorch_vision_detection.engine import train_one_epoch
from mask_rcnn_dataset import PennFudanDataset


def main_demo():
    train_gpu = torch.cuda.is_available()
    # 背景 + 行人
    num_classes = 2
    model = models.detection.maskrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=num_classes, pretrained_backbone=True)
    device = torch.device("cuda:0")
    model.to(device)

    dataset = PennFudanDataset("D:/datasets/PennFudanPed")
    data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)

    params = [p for p in model.parameters() if p.requires_grad]

    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
    num_epochs = 10
    for epoch in range(num_epochs):
        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)
        lr_scheduler.step()
    torch.save(model.state_dict(), "D:/models/mask_rcnn_model.pt")


if __name__ == '__main__':
    main_demo()
# demo1
import torchvision
import torch
import cv2 as cv
import numpy as np
coco_names = {'0': 'background', '1': 'person', '2': 'bicycle', '3': 'car', '4': 'motorcycle', '5': 'airplane', '6': 'bus',
         '7': 'train', '8': 'truck', '9': 'boat', '10': 'traffic light', '11': 'fire hydrant', '13': 'stop sign',
         '14': 'parking meter', '15': 'bench', '16': 'bird', '17': 'cat', '18': 'dog', '19': 'horse', '20': 'sheep',
         '21': 'cow', '22': 'elephant', '23': 'bear', '24': 'zebra', '25': 'giraffe', '27': 'backpack',
         '28': 'umbrella', '31': 'handbag', '32': 'tie', '33': 'suitcase', '34': 'frisbee', '35': 'skis',
         '36': 'snowboard', '37': 'sports ball', '38': 'kite', '39': 'baseball bat', '40': 'baseball glove',
         '41': 'skateboard', '42': 'surfboard', '43': 'tennis racket', '44': 'bottle', '46': 'wine glass',
         '47': 'cup', '48': 'fork', '49': 'knife', '50': 'spoon', '51': 'bowl', '52': 'banana', '53': 'apple',
         '54': 'sandwich', '55': 'orange', '56': 'broccoli', '57': 'carrot', '58': 'hot dog', '59': 'pizza',
         '60': 'donut', '61': 'cake', '62': 'chair', '63': 'couch', '64': 'potted plant', '65': 'bed',
         '67': 'dining table', '70': 'toilet', '72': 'tv', '73': 'laptop', '74': 'mouse', '75': 'remote',
         '76': 'keyboard', '77': 'cell phone', '78': 'microwave', '79': 'oven', '80': 'toaster', '81': 'sink',
         '82': 'refrigerator', '84': 'book', '85': 'clock', '86': 'vase', '87': 'scissors', '88': 'teddybear',
         '89': 'hair drier', '90': 'toothbrush'}

model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
model.eval()
transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])

train_gpu = torch.cuda.is_available()
if train_gpu:
    model.cuda()


def object_detection_video_demo():
    capture = cv.VideoCapture("D:/videos/Boogie_UP.mp4")
    while True:
        ret, frame = capture.read()
        if not ret:
            break
        blob = transform(frame)
        c, h, w = blob.shape
        input_x = blob.view(1, c, h, w)
        output = model(input_x.cuda())[0]
        boxes = output["boxes"].cpu().detach().numpy()
        scores = output["scores"].cpu().detach().numpy()
        labels = output['labels'].cpu().detach().numpy()
        index = 0
        for x1, y1, x2, y2 in boxes:
            if scores[index] > 0.5:
                cv.rectangle(frame, (np.int32(x1), np.int32(y1)), (np.int32(x2), np.int32(y2)),
                             (140, 199, 0), 2, 8, 0)
                label_id = labels[index]
                label_txt = coco_names[str(label_id)]
                cv.putText(frame, label_txt, (np.int32(x1), np.int32(y1)), cv.FONT_HERSHEY_SIMPLEX, 1.0, (30, 60, 120), 1)
                index += 1
            c = cv.waitKey(1)
        if c == ord('q'):
            break
            
        cv.imshow("video detection mask-rcnn", frame)

def instance_segmentation_demo():
    frame = cv.imread("D:/images/cat1.jpg")
    cv.imshow("input", frame)
    blob = transform(frame)
    c, h, w = blob.shape
    input_x = blob.view(1, c, h, w)
    output = model(input_x.cuda())[0]
    boxes = output["boxes"].cpu().detach().numpy()
    scores = output["scores"].cpu().detach().numpy()
    labels = output["labels"].cpu().detach().numpy()
    masks = output["masks"].cpu().detach().numpy()
    print("masks", masks.shape)
    index = 0
    color_mask = np.zeros((h, w, c), dtype=np.uint8)
    mv = cv.split(color_mask)


if __name__ == '__main__':
    object_detection_video_demo()
# demo2
import torch
import numpy as np
import cv2 as cv
from torchvision import models
from torchvision import transforms

model = models.detection.maskrcnn_resnet50_fpn(pretrained=False, progress=True, num_classes=2, pretrained_backbone=True)
model.load_state_dict(torch.load("D:/models/mask_rcnn_pedestrian_model.pt"))
model.eval()

transform = transforms.Compose([transforms.ToTensor()])

train_gpu = torch.cuda.is_available()

if train_gpu:
    model.cuda()

def object_detection_demo():
    frame = cv.imread("D:/images/gaoyy.png")
    cv.imshow("input", frame)
    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)
    blob = transform(frame)
    c, h, w = blob.shape
    print(c, h, w)
    input_x = blob.view(1, c, h, w)
    output = model(input_x.cuda())[0]
    boxes = output["boxes"].cpu().detach().numpy()
    scores = output["scores"].cpu().detach().numpy()
    labels = output["labels"].cpu().detach().numpy()
    masks = output["masks"].cpu().detach().numpy()
    index = 0
    frame = cv.cvtColor(frame, cv.COLOR_RGB2BGR)
    result_mask = np.zeros((h, w), dtype=np.uint8)
    for x1, y1, x2, y2 in boxes:
        print("score: ", scores[index])
        mask = np.reshape(masks[index], (h, w))
        mask[mask >= 0.5] = 255
        mask[mask < 0.5] = 0
        result_mask = cv.add(result_mask, np.uint8(mask))
        cv.rectangle(frame, (np.int32(x1), np.int32(y1)), (np.int32(x2), np.int32(y2)), (140, 199, 0), 2, 8, 0)
        index += 1
    result = cv.bitwise_and(frame, frame, mask=result_mask)
    cv.imshow("result", result)
    cv.imshow("not result mask", result_mask)

    mm = cv.imread("D:/images/Ped02.png")
    mm = cv.cvtColor(mm, cv.COLOR_BGR2RGB)
    blob = transform(mm)
    mc, mh, mw = blob.shape
    input_x = blob.view(1, mc, mh, mw)
    output = model(input_x.cuda())[0]
    boxes = output["boxes"].cpu().detach().numpy()
    scores = output["scores"].cpu().detach().numpy()
    labels = output["labels"].cpu().detach().numpy()
    masks = output["masks"].cpu().detach().numpy()
    index = 0
    mm = cv.cvtColor(mm, cv.COLOR_RGB2BGR)
    result_mask = np.zeros((mh, mw), dtype=np.uint8)
    for x1, y1, x2, y2 in boxes:
        print("score: ", scores[index])
        mask = np.reshape(masks[index], (mh, mw))
        mask[mask >= 0.5] = 255
        mask[mask < 0.5] = 0
        result_mask = cv.add(result_mask, np.uint8(mask))
        cv.rectangle(mm, (np.int32(x1), np.int32(y1)), (np.int32(x2), np.int32(y2)), (140, 199, 0), 2, 8, 0)
        index += 1
    res = cv.bitwise_and(mm, mm, mask=result_mask)
    cv.imshow("res", res)
    cv.bitwise_not(res, res)
    res = cv.bitwise_and(mm, mm, mask=res)
    cv.imshow("sub_res", res)
    result = cv.bitwise_and(res, res, mask=result_mask)
    cv.imshow("result2", result)
    cv.waitKey(0)

    cv.destroyAllWindows()


if __name__ == '__main__':
    object_detection_demo()
```

### 15.实战-UNet网络实现、训练与道路

```python
'''
图像语义分割
	支持多个类别
	像素级别的图像分类
语义分割网络
反卷积(转置卷积)与上采样
数据集名称：CrackForest Dataset
数据集分为groundTruth/image/seg
分别对应为：标注数据/图像数据/序列文本
'''
# utils
import numpy as np
import cv2 as cv
from scipy.io import loadmat
import os
def generate_mask():
    ground_dir = "D:/datasets/CrackForest-dataset/groundTruth"
    seg_dir = "D:/datasets/CrackForest-dataset/seg"
    files = os.listdir(ground_dir)
    sfiles = os.listdir(seg_dir)
    for i in range(len(files)):
        mat_file = os.path.join(ground_dir, files[i])
        seg_file = os.path.join(seg_dir, sfiles[i])
        mask_file_name = sfiles[i].replace(".seg", "")
        # 解析groundTruth
        m = loadmat(mat_file)
        mask = m["groundTruth"][0][0][1]
        # 解析seg
        file = open(seg_file)
        print(mat_file, seg_file)
        line = file.readline()
        while True:
            line = file.readline()
            segInfo = line.split(" ")
            if len(segInfo) == 4:
                c1 = np.int32(segInfo[2])
                c2 = np.int32(segInfo[3])
                seg_num = np.int32(segInfo[0])
                if seg_num == 1:
                    row = np.int32(segInfo[1])
                    for col in range(c1, c2, 1):
                        mask[row, col] = 1
            if not line:
                break
        file.close()
        # 形态学膨胀处理
        mask = np.uint8(mask * 255)
        se = cv.getStructuringElement(cv.MORPH_RECT, (3, 3))
        result = cv.morphologyEx(mask, cv.MORPH_CLOSE, se)
        cv.imwrite("D:/datasets/CrackForest-dataset/mask2/%s.png" %(mask_file_name), result)
if __name__ == '__main__':
    generate_mask()
    
# import torch
import numpy as np
import cv2 as cv
import os
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
class SegmentationDataset(Dataset):
    def __init__(self, image_dir, mask_dir):
        self.images = []
        self.masks = []
        files = os.listdir(image_dir)
        mask_files = os.listdir(mask_dir)
        for i in range(len(mask_files)):
            img_file = os.path.join(image_dir, files[i])
            mask_file = os.path.join(mask_dir, mask_files[i])
            self.images.append(img_file)
            self.masks.append(mask_file)
    def __len__(self):
        return len(self.images)

    def num_of_samples(self):
        return len(self.images)
    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            image_path = self.images[idx]
            mask_path = self.masks[idx]
        else:
            image_path = self.images[idx]
            mask_path = self.masks[idx]
        img = cv.imread(image_path, cv.IMREAD_GRAYSCALE)
        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)
        # 输入图像
        img = np.float32(img) / 255.0
        img = np.expand_dims(img, 0)
        # 目标标签0-1
        mask[mask <= 128] = 0
        mask[mask > 128] = 1
        mask = np.expand_dims(mask, 0)
        sample = {"image": torch.from_numpy(img), "mask": torch.from_numpy(mask)}
        return sample
if __name__ == '__main__':
    image_dir = "D:/datasets/CrackForest-dataset/image"
    mask_dir = "D:/datasets/CrackForest-dataset/mask"
    ds = SegmentationDataset(image_dir, mask_dir)
    dataloader = DataLoader(ds, batch_size=4, shuffle=True, num_workers=4)
    for i_batch, sample_batch in enumerate(dataloader):
        print(i_batch, sample_batch["image"].size(), sample_batch["mask"])
        break
       
# train
import torch
from torch.utils.data import DataLoader
from unet_dataset import SegmentationDataset
class UNetModel(torch.nn.Module):
    def __init__(self, in_features=1, out_features=2, init_features=32):
        super(UNetModel, self).__init__()
        features = init_features
        self.encode_layer1 = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=in_features, out_channels=features, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features),
            torch.nn.ReLU(),
            torch.nn.Conv2d(in_channels=features, out_channels=features, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features),
            torch.nn.ReLU()
        )
        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)
        self.encode_layer2 = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=features, out_channels=features * 2, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 2),
            torch.nn.ReLU(),
            torch.nn.Conv2d(in_channels=features * 2, out_channels=features * 2, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 2),
            torch.nn.ReLU()
        )
        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)
        self.encode_layer3 = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=features * 2, out_channels=features * 4, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 4),
            torch.nn.ReLU(),
            torch.nn.Conv2d(in_channels=features * 4, out_channels=features * 4, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 4),
            torch.nn.ReLU()
        )
        self.pool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)
        self.encode_layer4 = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=features * 4, out_channels=features * 8, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 8),
            torch.nn.ReLU(),
            torch.nn.Conv2d(in_channels=features * 8, out_channels=features * 8, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 8),
            torch.nn.ReLU(),
        )
        self.pool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)
        self.encode_decode_layer = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=features * 8, out_channels=features * 16, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 16),
            torch.nn.ReLU(),
            torch.nn.Conv2d(in_channels=features * 16, out_channels=features * 16, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 16),
            torch.nn.ReLU()
        )
        self.upconv4 = torch.nn.ConvTranspose2d(
            features * 16, features * 8, kernel_size=2, stride=2
        )
        self.decode_layer4 = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=features * 16, out_channels=features * 8, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 8),
            torch.nn.ReLU(),
            torch.nn.Conv2d(in_channels=features * 8, out_channels=features * 8, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 8),
            torch.nn.ReLU(),
        )
        self.upconv3 = torch.nn.ConvTranspose2d(
            features * 8, features * 4, kernel_size=2, stride=2
        )
        self.decode_layer3 = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=features * 8, out_channels=features * 4, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 4),
            torch.nn.ReLU(),
            torch.nn.Conv2d(in_channels=features * 4, out_channels=features * 4, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 4),
            torch.nn.ReLU()
        )
        self.upconv2 = torch.nn.ConvTranspose2d(
            features * 4, features * 2, kernel_size=2, stride=2
        )
        self.decode_layer2 = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=features * 4, out_channels=features * 2, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 2),
            torch.nn.ReLU(),
            torch.nn.Conv2d(in_channels=features * 2, out_channels=features * 2, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features * 2),
            torch.nn.ReLU()
        )
        self.upconv1 = torch.nn.ConvTranspose2d(
            features * 2, features, kernel_size=2, stride=2
        )
        self.decode_layer1 = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=features * 2, out_channels=features, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features),
            torch.nn.ReLU(),
            torch.nn.Conv2d(in_channels=features, out_channels=features, kernel_size=3, padding=1, stride=1),
            torch.nn.BatchNorm2d(num_features=features),
            torch.nn.ReLU()
        )
        self.out_layer = torch.nn.Sequential(
            torch.nn.Conv2d(in_channels=features, out_channels=out_features, kernel_size=1, padding=0, stride=1),
        )
    def forward(self, x):
        enc1 = self.encode_layer1(x)
        enc2 = self.encode_layer2(self.pool1(enc1))
        enc3 = self.encode_layer3(self.pool2(enc2))
        enc4 = self.encode_layer4(self.pool3(enc3))

        bottleneck = self.encode_decode_layer(self.pool4(enc4))
        dec4 = self.upconv4(bottleneck)
        dec4 = torch.cat((dec4, enc4), dim=1)
        dec4 = self.decode_layer4(dec4)

        dec3 = self.upconv3(dec4)
        dec3 = torch.cat((dec3, enc3), dim=1)
        dec3 = self.decode_layer3(dec3)

        dec2 = self.upconv2(dec3)
        dec2 = torch.cat((dec2, enc2), dim=1)
        dec2 = self.decode_layer2(dec2)

        dec1 = self.upconv1(dec2)
        dec1 = torch.cat((dec1, enc1), dim=1)
        dec1 = self.decode_layer1(dec1)

        out = self.out_layer(dec1)
        return out
def train_demo():
    train_gpu = torch.cuda.is_available()
    device = "cuda:0"
    unet = UNetModel(in_features=1, out_features=2, init_features=32)
    if train_gpu:
        unet.to(device)
    cross_loss = torch.nn.CrossEntropyLoss()
    best_validation = 0.0
    optimizer = torch.optim.Adam(unet.parameters(), lr=0.001)
    unet.train()
    num_epochs = 20
    image_dir = "D:/datasets/CrackForest-dataset/image"
    mask_dir = "D:/datasets/CrackForest-dataset/mask"
    ds = SegmentationDataset(image_dir, mask_dir)
    num_train_samples = ds.num_of_samples()
    dataloader = DataLoader(ds, batch_size=1, shuffle=True, num_workers=4)
    index = 0
    for epoch in range(num_epochs):
        train_loss = 0.0
        for i_batch, sample_batch in enumerate(dataloader):
            image_batch, target_labels = sample_batch["image"], sample_batch["mask"]
            if train_gpu:
                image_batch = image_batch.cuda()
                target_labels = target_labels.cuda()
            optimizer.zero_grad()
            label_out = unet(image_batch)
            target_labels = target_labels.contiguous().view(-1)
            label_out = label_out.transpose(1, 3).transpose(1, 2).contiguous().view(-1, 2)
            target_labels = target_labels.long()
            loss = cross_loss(label_out, target_labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            if index % 100 == 0:
                print("step {}\t Training Loss {:.6}".format(index, loss.item()))
            index += 1
        train_loss = train_loss / num_train_samples
        print("Epoch {}\tTraining Loss {:.6f}".format(epoch, train_loss))
    unet.eval()
    torch.save(unet, "D:/models/unet_road_mymodel.pt")
if __name__ == '__main__':
    train_demo()
    
# demo
import os
import torch
import cv2 as cv
import numpy as np
from unet_model import UNetModel
def unet_defect_demo():
    cnn_model = torch.load("D:/models/unet_road_model.pt")
    root_dir = "D:/datasets/CrackForest-dataset/test"
    fileNames = os.listdir(root_dir)
    for f in fileNames:
        image = cv.imread(os.path.join(root_dir, f), cv.IMREAD_GRAYSCALE)
        h, w = image.shape
        img = np.float32(image) / 255.0
        img = np.expand_dims(img, 0)
        x_input = torch.from_numpy(img).view(1, 1, h, w)
        probs = cnn_model(x_input.cuda())
        label_out = probs.transpose(1, 3).transpose(1, 2).contiguous().view(-1, 2)
        _, output = label_out.data.max(dim=1)
        output[output > 0] = 255
        predic_ = output.view(h, w).cpu().detach().numpy()
        print(predic_.shape)
        cv.imshow("input", image)
        result = cv.resize(np.uint8(predic_), (w, h))
        contours, h = cv.findContours(result, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
        bgr_img = cv.cvtColor(image, cv.COLOR_GRAY2BGR)
        cv.drawContours(bgr_img, contours, -1, (0, 0, 255), -1)
        cv.imshow("output", bgr_img)
        cv.waitKey(0)
    cv.destroyAllWindows()
if __name__ == '__main__':
    unet_defect_demo()
```

### 16.补充

```python
'''
衡量图像分类网络的性能top-1/top-5
衡量对象检测网络的mAP
衡量语义分割网络的评价指标mIU

'''
```

