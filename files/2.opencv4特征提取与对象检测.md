### 1.图像特征概述

```c++
/*
图像特征提取算法
	传统特征工程：SIFT/SURE/HOG/LPB/HAAR/ORB/AKAZE
	卷积神经网络：CNN/DNN
传统图像特征提取：
	基于纹理、角点、颜色分布、梯度、边缘等
深度卷积神经网络特征提取：
	基于监督学习、自动提取特征
特征数据/特征属性
	尺度空间不变性：图像放大缩小
	像素迁移不变性：
	光照一致性原则：亮度调暗调亮
	旋转不变性原则：图像旋转
图像特征应用
	图像分类
	对象识别
	图像搜索/对比
	对象检测
	特征检测
	图像对齐/匹配
*/
```

### 2.角点检测

```c++
/*
Harris角点检测算法：R = det(M)-k*trace(M)**2 = x1*x2 - k * (x1 + x2)**2
cornerHarris(
	inputArray src,  # 输入 
	OutputArray dst,  # 输出
	int blockSize,  # 块大小
	int ksize,  # Soble
	double k,  # 常量系数
	int borderType=BORDER_DEAFAULT)
Shi-tomas角点检测算法: R = min(x1, x2)
goodFeaturesToTrack(
	InputArray image,   # 输入图像
	OutputArray corners,  # 输出的角点坐标
	int maxCorners,  # 最大角点数目
	double qualityLevel,  # 质量控制(假设为0.15，最大值*0.15得出一个阈值，低于这个阈值就忽略)
	double minDistance,  # 重叠控制(设置两个角点之间的距离, 小于这个距离认为是同一个角点)
	InputArray mask=noArray(),
	int blockSize=3，  # 同minDistance
	bool useHarrisDetector=False,
	double k = 0.04
)
*/
Mat gray;
cvtColor(src, gray, COLOR_BGR2GRAY);
vector<Point>corners;
goodFeaturesToTrack(gray, corners, 400, 0.015, 10);
RNG rng;
for (size_t t = 0; t < corners.size(); t++) {
    int b = rng.uniform(0, 255);
    int g = rng.uniform(0, 255);
    int r = rng.uniform(0, 255);
    //circle(src, corners[t], 2, Scalar(0, 0, 0), 2, 8, 0);
    circle(src, corners[t], 4, Scalar(b, g, r), 2, 8, 0);
}
```

### 3.ORB关键点检测

```c++
/*
比SIFT与SURF速度快
Fast关键点检测：
	选择当前像素点P，阈值T，周围16个像素点，超过连续N=12个像素点大于或者小于P
Fast1：优先检测1、5、9、13像素点
循环所有像素点
cv::ORB::create(500)
cv::Feature2D::detect(
	InputArray image,  // 输入图像
	std::vector<KeyPoint>&keypoints,  // 关键点
	InputArray mask=noArray(),  // 支持mask，提取图像上的某个区域
)
KeyPoint四个重要属性：
	pt   坐标
	angle 角度
	response  响应
	size 半径
*/
auto orb = ORB::create(500);
vector<KeyPoint>kypts;
orb->detect(image, kypts);
Mat result;
drawKeypoints(image, kypts, result, Scalar::all(-1),DrawMatchesFlags::DRAW_RICH_KEYPOINTS);
```

### 4.ORB特征描述子

```c++
/*
基于关键点周围区域
浮点数表示与二值编码
描述子长度
SIFT：128Bytes
ORB：32Bytes
ORB特征描述子步骤：
	提取特征关键点
	描述子方向指派
	特征描述子编码(二值编码32位)
	几何矩公式、计算中心位置、计算角度
	变换矩阵M、像素块坐标矩阵、旋转矩阵
*/
// 特征描述子
Mat desription;
orb->compute(image, kypts, desription);
// desription.cols 关键点的描述子长度32
std::cout << desription.rows << "x" << desription.cols << std::endl;
for (size_t t = 0; t < desription.rows; t++) {
    if (t % 100 == 0) {
        std::cout << desription.ptr<int>(t) << std::endl;
    }
}
```

### 5.SIFT特征描述子

```c++
/*
步骤：
	尺度空间极值检测：构建尺度空间-图像金字塔+高斯尺度空间、三层(bgr)空间中的极值查找
	关键点定位：极值点定位-求导拟合、删除低对比度与低相应候选点
	关键点方向指派：Scale尺度最近的图像，1.5倍大小的高斯窗口
	特征描述子：128维度向量/特征描述子，分成16个每个8个方向
ORB比SIFT速递快
*/
auto sift = SIFT::create(500);
vector<KeyPoint>kypts;
sift->detect(image, kypts);
Mat result;
drawKeypoints(image, kypts, result, Scalar::all(-1), DrawMatchesFlags::DRAW_RICH_KEYPOINTS);

// 特征描述子
Mat desription;
sift->compute(image, kypts, desription);
// desription.cols 关键点的描述子长度128
std::cout << desription.rows << "x" << desription.cols << std::endl;
std::cout << kypts.size() << std::endl;
for (int i = 0; i < kypts.size(); i++) {
    std::cout << " pt: " << kypts[i].pt << " angle: " << kypts[i].angle << " size: " << kypts[i].size << std::endl;
}
for (size_t t = 0; t < desription.rows; t++) {
    if (t % 100 == 0) {
        std::cout << desription.ptr<int>(t) << std::endl;
    }
}
```

### 6.暴力与flann特征匹配

```c++
/*
特征匹配算法
	暴力匹配：全局搜索，计算最小距离，返回相似描述子合集
	FLANN匹配：高维数据匹配算法库
SIFT不支持flann匹配，基于LI Norm
ORB支持flann匹配，基于Hanming distance
特征匹配DMatch数据
	queryidx
	trainidx
	distance
	distance表示距离，值越小表示匹配程度越高
*/
// 暴力匹配
auto orb = ORB::create(500);
vector<KeyPoint>kypts_book;
vector<KeyPoint>kypts_book_on_desk;
Mat desc_book, desc_book_on_desk;
// 同时提取关键点和计算描述子
orb->detectAndCompute(book,Mat(), kypts_book, desc_book);
orb->detectAndCompute(book_on_desk,Mat(), kypts_book_on_desk, desc_book_on_desk);
Mat result;
// 创建匹配者
auto bf_matcher = BFMatcher::create(NORM_HAMMING, false);
// 匹配的结果
vector<DMatch>matches;
// 进行匹配
bf_matcher->match(desc_book, desc_book_on_desk, matches);
drawMatches(book, kypts_book, book_on_desk, kypts_book_on_desk, matches, result);

// FLANN匹配
void flann_demo(Mat& book, Mat& book_on_desk) {
	imshow("book", book);
	auto orb = ORB::create(500);
	vector<KeyPoint>kypts_book;
	vector<KeyPoint>kypts_book_on_desk;
	Mat desc_book, desc_book_on_desk;
	//同时提取关键点和计算描述子
	orb->detectAndCompute(book, Mat(), kypts_book, desc_book);
	orb->detectAndCompute(book_on_desk, Mat(), kypts_book_on_desk, desc_book_on_desk);
	Mat result;
	// 创建flann匹配者
	auto flannMatcher = FlannBasedMatcher(new flann::LshIndexParams(6, 12, 2));
	// 匹配的结果
	std::vector<DMatch>matches;
	flannMatcher.match(desc_book, desc_book_on_desk, matches);
	cout << "matches=" << matches.size() << endl;
	drawMatches(book, kypts_book, book_on_desk, kypts_book_on_desk, matches, result);
	imshow("flann匹配结果", result);
}
// sift+flann
void sift_flann_demo(Mat& book, Mat& book_on_desk) {
	imshow("book", book);
	auto st = SIFT::create(500);
	vector<KeyPoint>kypts_book;
	vector<KeyPoint>kypts_book_on_desk;
	Mat desc_book, desc_book_on_desk;
	st->detectAndCompute(book, Mat(), kypts_book, desc_book);
	st->detectAndCompute(book_on_desk, Mat(), kypts_book_on_desk, desc_book_on_desk);
	Mat result;
	auto flannMatcher = FlannBasedMatcher();
	vector<DMatch>matches;
	flannMatcher.match(desc_book, desc_book_on_desk, matches);
	drawMatches(book, kypts_book, book_on_desk, kypts_book_on_desk, matches, result);
	imshow("sift+flann", result);
}
```

### 7.单映射矩阵与透视变换

```c++
/*
findHomography()  
拟合方法：
	最小二乘
	随机采样一致性(RANSC)
	渐进采样一致性(RHO)
*/
cvtColor(image, gray, COLOR_BGR2GRAY);
threshold(gray, binary, 0, 255, THRESH_BINARY_INV | THRESH_OTSU);
imshow("binary", binary);
vector<vector<Point>>contours;
vector<Vec4i>hierachy;
findContours(binary, contours, hierachy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE);
int index = -1;
double max = -1;
for (int i = 0; i < contours.size(); i++) {
    double area = contourArea(contours[i]);
    if (area > max) {
        index = i;
        max = area;
    }
}
cout << "index:" << index << endl;
drawContours(image, contours, index, Scalar(0, 255, 0), 2, 8);
Mat approxCurves;
vector<Point2f>srcPts;
// 轮廓逼近
approxPolyDP(contours[index], approxCurves, 100, true);
cout << "contours.size(): " << contours.size() << endl;
for (int i = 0; i < contours.size(); i++) {
    Vec2i pt = approxCurves.at<Vec2i>(i, 0);
    std::cout << pt << std::endl;
    srcPts.push_back(Point2f(pt[0], pt[1]));
    circle(image, Point(pt[0], pt[1]), 12, Scalar(0, 0, 255), 2, 8, 0);
}
vector<Point2f>dstPts;
// 需要和pt的点一一对应
dstPts.push_back(Point2f(300, 0));
dstPts.push_back(Point2f(0, 0));
dstPts.push_back(Point2f(0, 450));
dstPts.push_back(Point2f(400, 450));
imshow("轮廓", image);
// 找到两个平面之间的转换矩阵, 计算多个二维点对之间的最优单映射变换矩阵
Mat h = findHomography(srcPts, dstPts, RANSAC);
Mat dst;
// 透视变换
warpPerspective(image, dst, h, Size(300, 450));
```

### 8.对象检测与发现

```c++
/*
基于特征的匹配与对象检测
ORB/AKAZE/SIFT
暴力/FLANN
透视变换
检测框
*/
auto orb = ORB::create(500);
vector<KeyPoint>kypts_book;
vector<KeyPoint>kypts_book_on_desk;
Mat desc_book, desc_book_on_desk;
// 同时提取关键点和计算描述子
orb->detectAndCompute(book, Mat(), kypts_book, desc_book);
orb->detectAndCompute(book_on_desk, Mat(), kypts_book_on_desk, desc_book_on_desk);
Mat result;
// 创建匹配者
auto bf_matcher = BFMatcher::create(NORM_HAMMING, false);
// 匹配的结果
vector<DMatch>matches;
vector<Point2f>obj_pts;
vector<Point2f>scene_pts;
// 进行匹配
bf_matcher->match(desc_book, desc_book_on_desk, matches);
float good_rate = 0.15f;
int num_good_matches = matches.size() * good_rate;
std::cout << num_good_matches << std::endl;
std::sort(matches.begin(), matches.end());
// 把检测出来不靠谱的匹配擦除
matches.erase(matches.begin() + num_good_matches, matches.end());
drawMatches(book, kypts_book, book_on_desk, kypts_book_on_desk, matches, result);
// 返回关键点
for (size_t t = 0; t < matches.size(); t++) {
    obj_pts.push_back(kypts_book[matches[t].queryIdx].pt);
    scene_pts.push_back(kypts_book_on_desk[matches[t].trainIdx].pt);
}
// 求出变换矩阵
Mat h = findHomography(obj_pts, scene_pts, RANSAC);
vector<Point2f>srcPts;
srcPts.push_back(Point2f(0, 0));
srcPts.push_back(Point2f(book.cols, 0));
srcPts.push_back(Point2f(book.cols, book.rows));
srcPts.push_back(Point2f(0, book.rows));
std::vector<Point2f>scene_corners(4);
// 计算出变换后点四个角点
perspectiveTransform(srcPts, scene_corners, h);
for (int i = 0; i < 4; i++) {
    line(book_on_desk, scene_corners[i], scene_corners[(i + 1) % 4], Scalar(0, 0, 255), 2, 8, 0);
}
```

### 9.文档对齐

```c++
/*
模板表单/文档
特征匹配与对齐
*/
Mat ref_img = imread("D:/images/case1.png");
Mat img = imread("D:/images/case1r.png");
imshow("表单模板", ref_img);
auto orb = ORB::create(500);
vector<KeyPoint>kypts_ref;
vector<KeyPoint>kypts_img;
Mat desc_book, desc_book_on_desk;
orb->detectAndCompute(ref_img, Mat(), kypts_ref, desc_book);
orb->detectAndCompute(img, Mat(), kypts_img, desc_book_on_desk);
Mat result;
auto bf_matcher = BFMatcher::create(NORM_HAMMING, false);
vector<DMatch>matches;
bf_matcher->match(desc_book_on_desk, desc_book, matches);
float good_rate = 0.15f;
int num_good_matches = matches.size() * good_rate;
std::cout << num_good_matches << std::endl;
std::sort(matches.begin() + num_good_matches, matches.end());
matches.erase(matches.begin() + num_good_matches, matches.end());
drawMatches(ref_img, kypts_ref, img, kypts_img, matches, result);
imshow("匹配", result);

std::vector<Point2f>points1, points2;
for (size_t i = 0; i < matches.size(); i++) {
    points1.push_back(kypts_img[matches[i].queryIdx].pt);
    points2.push_back(kypts_ref[matches[i].trainIdx].pt);
}
Mat h = findHomography(points1, points2, RANSAC);
Mat aligned_doc;
warpPerspective(img, aligned_doc, h, ref_img.size());
imshow("align_doc", aligned_doc);
```

### 10.图像拼接

```c++
/*
特征检测与匹配
图像对齐与变换
图像边缘融合
Mask生成-generateMask
梯度边缘融合-linspace
*/
void linspace(Mat& image, float begin, float finish, int number, Mat& mask);
void generate_mask(Mat& img, Mat& mask);
int roi_demo(Mat& image);

int main(int argc, char** argv) {
	Mat left = imread("D:/images/left1.jpg");
	Mat right = imread("D:/images/right1.jpg");
	if (left.empty() || right.empty()) {
		printf("could not load image...");
		return -1;
	}
	// 提取特征点与描述子
	vector<KeyPoint>keypoints_right, keypoints_left;
	Mat descriptors_right, descriptors_left;
	auto detector = AKAZE::create();
	detector->detectAndCompute(left, Mat(), keypoints_left, descriptors_left);
	detector->detectAndCompute(right, Mat(), keypoints_right, descriptors_right);

	// 暴力匹配
	vector<DMatch> matches;
	auto matcher = DescriptorMatcher::create(DescriptorMatcher::BRUTEFORCE);

	// 发现匹配
	std::vector<std::vector<DMatch>>knn_matches;
	matcher->knnMatch(descriptors_left, descriptors_right, knn_matches, 2);
	const float ratio_tresh = 0.7f;
	std::vector<DMatch> good_matches;
	for (size_t i = 0; i < knn_matches.size(); i++) {
		if (knn_matches[i][0].distance < ratio_tresh * knn_matches[i][1].distance) {
			good_matches.push_back(knn_matches[i][0]);
		}
	}
	printf("total good_match points: %d\n", good_matches.size());
	std::cout << std::endl;
	Mat dst;
	drawMatches(left, keypoints_left, right, keypoints_right, good_matches, dst);
	imshow("output", dst);
	
	std::vector<Point2f> left_pts;
	std::vector<Point2f> right_pts;

	for (size_t i = 0; i < good_matches.size(); i++) {
		// 收集所有好的匹配点
		left_pts.push_back(keypoints_left[good_matches[i].queryIdx].pt);
		right_pts.push_back(keypoints_right[good_matches[i].trainIdx].pt);
	}

	// 配准与对齐，对齐到第一张
	Mat H = findHomography(right_pts, left_pts, RANSAC);

	// 获取全景图大小
	int h = max(left.rows, right.rows);
	int w = left.cols + right.cols;
	Mat panorama_01 = Mat::zeros(Size(w, h), CV_8UC3);
	Rect roi;
	roi.x = 0;
	roi.y = 0;
	roi.width = left.cols;
	roi.height = left.rows;

	// 获取左侧与右侧对齐图像
	left.copyTo(panorama_01(roi));
	imshow("panorama_01", panorama_01);
	Mat panorama_02;
	warpPerspective(right, panorama_02, H, Size(w, h));

	// 计算融合重叠区域mask
	Mat mask = Mat::zeros(Size(w, h), CV_8UC1);
	generate_mask(panorama_02, mask);

	// 创建遮罩层并根据mask完成权重初始化
	Mat mask1 = Mat::ones(Size(w, h), CV_32FC1);
	Mat mask2 = Mat::ones(Size(w, h), CV_32FC1);

	// left mask
	linspace(mask1, 1, 0, left.cols, mask);

	// right mask
	linspace(mask2, 0, 1, left.cols, mask);

	//imshow("mask1", mask1);
	//imshow("mask2", mask2);

	// 左侧融合
	Mat m1;
	vector<Mat> mv;
	mv.push_back(mask1);
	mv.push_back(mask1);
	mv.push_back(mask1);
	merge(mv, m1);
	panorama_01.convertTo(panorama_01, CV_32F);
	multiply(panorama_01, m1, panorama_01);

	// 右侧融合
	mv.clear();
	mv.push_back(mask2);
	mv.push_back(mask2);
	mv.push_back(mask2);
	Mat m2;
	merge(mv, m2);
	panorama_02.convertTo(panorama_02, CV_32F);
	multiply(panorama_02, m2, panorama_02);

	// 合并全景图
	Mat panorama;
	imshow("panorama_01", panorama_01);
	imshow("panorama_02", panorama_02);
	add(panorama_01, panorama_02, panorama);
	panorama.convertTo(panorama, CV_8U);
	int roi_w = roi_demo(panorama);
	cout << "roi_w: " <<roi_w << endl;
	Mat crop = panorama(Range(0, panorama.rows), Range(0, panorama.cols - roi_w));
	imshow("panorama", crop);
	imwrite("D:/images/panorama01.jpg", crop);

	waitKey(0);
	return 0;
}

// 截取图像
int roi_demo(Mat& image) {
	int w = image.cols;
	int h = image.rows;
	int roi_h = 0;
	int roi_w = 0;
	for (int col = 0; col < w; col++) {
		int roi_h = 0;
		for (int row = 0; row < h; row++) {
			Vec3b p = image.at<Vec3b>(row, col);
			int b = p[0];
			int g = p[1];
			int r = p[2];
			if (b == g && g == r && r == 0) {
				roi_h++;
			}
		}
		if (roi_h == h) {
			roi_w++;
		}
	}
	return roi_w;
}

void generate_mask(Mat& img, Mat& mask) {
	int w = img.cols;
	int h = img.rows;
	for (int row = 0; row < h; row++) {
		for (int col = 0; col < w; col++) {
			Vec3b p = img.at<Vec3b>(row, col);
			int b = p[0];
			int g = p[1];
			int r = p[2];
			if (b == g && g == r && r == 0) {
				mask.at<uchar>(row, col) = 255;
			}
		}
	}
}

void linspace(Mat& image, float begin, float finish, int w1, Mat& mask) {
	int offsetx = 0;
	float interval = 0;
	float delta = 0;
	for (int i = 0; i < image.rows; i++) {
		offsetx = 0;
		interval = 0;
		delta = 0;
		for (int j = 0; j < image.cols; j++) {
			int pv = mask.at<uchar>(i, j);
			if (pv == 0 && offsetx == 0) {
				offsetx = j;
				delta = w1 - offsetx;
				interval = (finish - begin) / (delta - 1);
				image.at<float>(i, j) = begin + (j - offsetx) * interval;
			}
			else if (pv == 0 && offsetx > 0 && (j - offsetx) < delta) {
				image.at<float>(i, j) = begin + (j - offsetx) * interval;
			}
		}
	}
}
```

### 11.条码标签定位与有无判定

```c++
/*
特征检测与匹配
图像对齐与变换
判断检查结果，分割标签
*/
// ORBDetector.h
#pragma once
#include<opencv2/opencv.hpp>

class ORBDetector {
public:
	ORBDetector(void);
	~ORBDetector(void);
	void initORB(cv::Mat& refImg);
	bool detect_and_analysis(cv::Mat& image, cv::Mat& aligned);
private:
	cv::Ptr<cv::ORB>orb = cv::ORB::create(500);
	std::vector<cv::KeyPoint> tpl_kps;
	cv::Mat tpl_descriptors;
	cv::Mat tpl;
};

// ORBDetector.cpp
#include "ORBDetector.h"

ORBDetector::ORBDetector() {
	std::cout << "create orb detector..." << std::endl;
}
ORBDetector::~ORBDetector() {
	this->tpl_descriptors.release();
	this->tpl_kps.clear();
	this->orb.release();
	this->tpl.release();
	std::cout << "destory instance..." << std::endl;
}

void ORBDetector::initORB(cv::Mat& refImg) {
	if (!refImg.empty()) {
		cv::Mat tplGray;
		cv::cvtColor(refImg, tplGray, cv::COLOR_BGR2GRAY);
		orb->detectAndCompute(tplGray, cv::Mat(), this->tpl_kps, this->tpl_descriptors);
		tplGray.copyTo(this->tpl);
	}
}

bool ORBDetector::detect_and_analysis(cv::Mat& image, cv::Mat& aligned) {
	// keypoints and match threshold
	float GOOD_MATCH_PERCENT = 0.15f;
	bool found = true;
	// 处理数据集中每一张数据
	cv::Mat img2Gray;
	cv::cvtColor(image, img2Gray, cv::COLOR_BGR2GRAY);
	std::vector<cv::KeyPoint> img_kps;
	cv::Mat img_descriptors;
	orb->detectAndCompute(img2Gray, cv::Mat(), img_kps, img_descriptors);

	std::vector<cv::DMatch> matches;
	cv::Ptr<cv::DescriptorMatcher> matcher = cv::DescriptorMatcher::create("BruteForce-Hamming");
	matcher->match(img_descriptors, this->tpl_descriptors, matches, cv::Mat());
	std::sort(matches.begin(), matches.end());

	const int numGoodMatches = matches.size() * GOOD_MATCH_PERCENT;
	matches.erase(matches.begin() + numGoodMatches, matches.end());
	if (matches[0].distance > 30) {
		found = false;
	}
	std::vector<cv::Point2f>points1, points2;
	for (size_t i = 0; i < matches.size(); i++) {
		points1.push_back(img_kps[matches[i].queryIdx].pt);
		points2.push_back(tpl_kps[matches[i].trainIdx].pt);
	}
	cv::Mat H = findHomography(points1, points2, cv::RANSAC);
	cv::Mat im2Reg;
	warpPerspective(image, im2Reg, H, tpl.size());

	// 逆时针旋转90度
	cv::Mat result;
	cv::rotate(im2Reg, result, cv::ROTATE_90_COUNTERCLOCKWISE);
	result.copyTo(aligned);
	return found;
}

// demo
#include<ORBDetector.h>
#include<iostream>

using namespace cv;
using namespace std;

int main(int argc, char** argv) {
	Mat refImg = imread("D:/images/tpl2.png");
	ORBDetector orb_detector;
	orb_detector.initORB(refImg);
	vector<std::string>files;
	glob("D:/images/orb_barcode", files);
	cv::Mat temp;
	for (auto file : files) {
		std::cout << file << std::endl;
		cv::Mat image = imread(file);
		int64 start = getTickCount();
		bool OK = orb_detector.detect_and_analysis(image, temp);
		double ct = (getTickCount() - start) / getTickFrequency();
		printf("decode time: %.5f ms\n", ct * 1000);
		std::cout << "标签" << (OK == true) << std::endl;
		imshow("temp", temp);
		waitKey(0);
	}
}
```

### 12.DNN概述与SSD对象检测

```C++
/*
opencv本身只支持推理，不支持训练
DNN模型
	DNN-Deep Neural Network
	支持VOC与COCO数据集的对象检测模型
	包括SSD/Faster-RCNN/YOLOV4等
	支持自定义对象检测
	支持人脸检测， HAAR人脸检测已弃用
	支持openvino加速
	支持tensorflow/caffe/onnx类型
	缺点：支持的模型数量有限/类型有限opencv目录下的\sources\samples\dnn
	models.yaml  opencv安装目录下
SSD的输出
	1*1*N*7 -DetectOutput  N不会超过300
	[image_id, label, conf, x_min, y_min, x_max, y_max]
	x_min,y_min,x_max, y_max 不是真实的宽高坐标，是0~1之间的数，需要乘以图像的宽高
相应的函数
	Net net = readNetFromTensorflow(model, config)
	Net net = readNetFromCaffe(config, model)
	Net net = readNetFromONNX(onnxfile)
在opencv目录下的\sources\samples\dnn用cmd执行下面代码, 然后就可以使用tensorflow来预测
tf_text_graph_ssd.py tf_text_graph_faster_rcnn.py --input frozen_inference_graph.pb --output frozen_inference_graph.pbtxt --config pipline.config
*/
#include<opencv2/opencv.hpp>
#include<iostream>
#include<opencv2/dnn.hpp>

using namespace cv;
using namespace cv::dnn;
using namespace std;

String objNames[] = { "background",
"aeroplane", "bicycle", "bird", "boat",
"bottle", "bus", "car", "cat", "chair",
"cow", "diningtable", "dog", "horse",
"motorbike", "person", "pottedplant",
"sheep", "sofa", "train", "tvmonitor" };

int main(int argc, char** argv) {
	Mat frame = imread("D:/images/cat-1.jpg");
	if (frame.empty()) {
		printf("could not load image ...\n");
		return 0;
	}
	namedWindow("input image", WINDOW_AUTOSIZE);
	imshow("input image", frame);
	std::string model_text_file = "D:/project/vsworkspaces/opencv4_feature_tutorial/dnn_ssd_demo/MobileNetSSD_deploy.prototxt.txt";
	std::string modelFile = "D:/project/vsworkspaces/opencv4_feature_tutorial/dnn_ssd_demo/MobileNetSSD_deploy.caffemodel";
	Net net = readNetFromCaffe(model_text_file, modelFile);
	Mat blobImage = blobFromImage(frame, 0.007843, Size(300, 300), Scalar(127.5, 127.5, 127.5), false, false);
	printf("blobImage width : %d, height: %d\n", blobImage.cols, blobImage.rows);
	net.setInput(blobImage, "data");
    // 返回四维前面两维是1，1需要的是第三和四维
	Mat detection = net.forward("detection_out");

	// post-process  detection.size[2]获取第三维数据
    // 构建detection.size[2]行，detection.size[3]列的数据
	Mat detectionMat(detection.size[2], detection.size[3], CV_32F, detection.ptr<float>());
	float confidence_threshold = 0.5;
	for (int i = 0; i < detectionMat.rows; i++) {
        // 获取每一行的数据
		float* curr_row = detectionMat.ptr<float>(i);
		int image_id = (int)(*curr_row++);
		size_t objIndex = (size_t)(*curr_row++);
		float score = *curr_row++;
		if (score > confidence_threshold) {
			float t1_x = (*curr_row++) * frame.cols;
			float t1_y = (*curr_row++) * frame.rows;
			float br_x = (*curr_row++) * frame.cols;
			float br_y = (*curr_row++) * frame.rows;
			Rect object_box((int)t1_x, (int)t1_y, (int)(br_x)-t1_x, (int)(br_y - t1_y));
			rectangle(frame, object_box, Scalar(0, 0, 255), 2, 8, 0);
			putText(frame, format("confidence %.2f, %s", score, objNames[objIndex].c_str()), Point(t1_x - 10, t1_y - 5), FONT_HERSHEY_SIMPLEX, 0.7, Scalar(255, 0, 0), 2, 8);
		}
	}
	imshow("ssd-demo", frame);
	waitKey(0);
	return 0;
}
```

### 13.Faster-RCNN对象检测

```c++
/*
在opencv目录下的\sources\samples\dnn用cmd执行下面代码, 然后就可以使用tensorflow来预测
tf_text_graph_faster_rcnn.py --input frozen_inference_graph.pb --output frozen_inference_graph.pbtxt --config pipline.config
*/
string label_map = "D:/python/tensorflow/mscoco_label_map.pbtxt";
string model = "D:/project/vsworkspaces/opencv4_feature_tutorial/faster_rcnn_demo/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb";
string config = "D:/project/vsworkspaces/opencv4_feature_tutorial/faster_rcnn_demo/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pbtxt";

std::map<int, String>readLabelMaps();
int main(int argc, char** argv) {
	Mat src = imread("D:/images/metro-people.png");
	int width = src.cols;
	int height = src.rows;
	if (src.empty()) {
		printf("could not load image...\n");
		return 0;
	}
	namedWindow("input", WINDOW_AUTOSIZE);
	imshow("input", src);
	map<int, string>names = readLabelMaps();
	printf("done read lables \n");

	// 加载Faster-RCNN
	Net net = readNetFromTensorflow(model, config);
	net.setPreferableBackend(DNN_BACKEND_OPENCV);
	net.setPreferableTarget(DNN_TARGET_CPU);

	// 设置输入
	Mat blob = blobFromImage(src, 1.0, Size(800, 600), Scalar(), true, false);
	net.setInput(blob);

	// 预测
	Mat detection = net.forward();
	Mat detectionMat(detection.size[2], detection.size[3], CV_32F, detection.ptr<float>());
	float threshold = 0.85;

	// 处理输出数据，绘制预测框与文本
	for (int row = 0; row < detectionMat.rows; row++) {
		float confidence = detectionMat.at<float>(row, 2);
		if (confidence > threshold) {
			// base zero
			int object_class = detectionMat.at<float>(row, 1) + 1;

			// predict box
			int left = detectionMat.at<float>(row, 3) * width;
			int top = detectionMat.at<float>(row, 4) * height;
			int right = detectionMat.at<float>(row, 5) * width;
			int bottom = detectionMat.at<float>(row, 6) * height;
			Rect rect;
			rect.x = left;
			rect.y = top;
			rect.width = (right - left);
			rect.height = (bottom - top);

			// render bounding box and label name
			rectangle(src, rect, Scalar(255, 0, 255), 1, 8, 0);
			map<int, string>::iterator it = names.find(object_class);
			printf("id: %d, display name: %s \n", object_class, (it->second).c_str());
			putText(src, (it->second).c_str(), Point(left, top - 5), FONT_HERSHEY_SIMPLEX, 0.5, Scalar(255, 0, 0), 1);
		}
	}
	imshow("faster-rcnn-demo", src);
	waitKey(0);
	return 0;
}

std::map<int, string> readLabelMaps()
{
	std::map<int, string> labelNames;
	std::ifstream fp(label_map);
	if (!fp.is_open())
	{
		printf("could not open file...\n");
		exit(-1);
	}
	string one_line;
	string display_name;
	while (!fp.eof())
	{
		std::getline(fp, one_line);
		std::size_t found = one_line.find("id:");
		if (found != std::string::npos) {
			int index = found;
			string id = one_line.substr(index + 4, one_line.length() - index);

			std::getline(fp, display_name);
			std::size_t  found = display_name.find("display_name:");

			index = found + 15;
			string name = display_name.substr(index, display_name.length() - index);
			name = name.replace(name.length() - 1, name.length(), "");
			// printf("id : %d, name: %s \n", stoi(id.c_str()), name.c_str());
			labelNames[stoi(id)] = name;
		}
	}
	fp.close();
	return labelNames;
}
```

### 14.YOLOv4对象检测

```c++
/*
YOLOv5不支持opencv，支持openvino
解析多个输出层，80个分类N*w*h*d
4+80预测，三个输出层，YOLOv5合为一个输出
centerx, centery, width, height
13*13*255 = 13*13*3*85
NMS非最大抑制no-max suppression
https://github.com/AlexeyAB/darknet/wiki/YOLOv4-model-zoo
*/
void image_detection();
string yolov4_model = "D:/project/vsworkspaces/opencv4_feature_tutorial/dnn_yolov4_demo/yolov4.weights";
string yolov4_config = "D:/project/vsworkspaces/opencv4_feature_tutorial/dnn_yolov4_demo/yolov4.cfg";

int main(int argc, char** argv)
{
	image_detection();
}

void image_detection() {
	Net net = readNetFromDarknet(yolov4_config, yolov4_model);
	net.setPreferableBackend(DNN_BACKEND_CUDA);
	net.setPreferableTarget(DNN_TARGET_CUDA);

	vector<string> outNames = net.getUnconnectedOutLayersNames();
	for (int i = 0; i < outNames.size(); i++) {
		printf("output layer name: %s\n", outNames[i].c_str());
	}

	vector<string>classNamesVec;
	ifstream classNamesFile("D:/project/vsworkspaces/opencv4_feature_tutorial/dnn_yolov4_demo/object_detection_classes_yolov4.txt");
	if (classNamesFile.is_open()) {
		string className = "";
		// 读取文件的每行，并赋值给className
		while (getline(classNamesFile, className))
			classNamesVec.push_back(className);
	}

	VideoCapture capture;
	capture.open("D:/videos/Boogie_UP.mp4");
	Mat frame;

	while (true) {
		// 获取时间，计时
		int64 start = getTickCount();
		capture.read(frame);
		// 整体像素值减去平均值，通过缩放系数对图片像素值进行缩放, bgr转为gbr  ---true
		Mat inputBlob = blobFromImage(frame, 1 / 255.F, Size(416, 416), Scalar(), true, false);
		net.setInput(inputBlob);

		// 检测
		vector<Mat> outs;
		net.forward(outs, outNames);

		vector<Rect>boxes;
		vector<int> classIds;
		vector<float> confidences;
		for (size_t i = 0; i < outs.size(); ++i) {
			float* data = (float*)outs[i].data;
			for (int j = 0; j < outs[i].rows; ++j, data += outs[i].cols) {
				Mat scores = outs[i].row(j).colRange(5, outs[i].cols);
				Point classIdPoint;
				double confidence;
				minMaxLoc(scores, 0, &confidence, 0, &classIdPoint);
				if (confidence > 0.5) {
					int centerX = (int)(data[0] * frame.cols);
					int centerY = (int)(data[1] * frame.rows);
					int width = (int)(data[2] * frame.cols);
					int height = (int)(data[3] * frame.rows);
					// 获取左上顶点的位置
					int left = centerX - width / 2;
					int top = centerY - height / 2;

					classIds.push_back(classIdPoint.x);
					confidences.push_back((float)confidence);
					boxes.push_back(Rect(left, top, width, height));
				}
			}
		}
		vector<int> indices;
		// 非最大抑制
		NMSBoxes(boxes, confidences, 0.5, 0.2, indices);
		for (size_t i = 0; i < indices.size(); i++) {
			int idx = indices[i];
			Rect box = boxes[idx];
			String className = classNamesVec[classIds[idx]];
			putText(frame, className.c_str(), box.tl(), FONT_HERSHEY_SIMPLEX, 1.0, Scalar(255, 0, 0), 2, 8);
			rectangle(frame, box, Scalar(0, 0, 255), 2, 8, 0);

		}
		float fps = getTickFrequency() / (getTickCount() - start);
		float time = (getTickCount() - start) / getTickFrequency();
		ostringstream ss;
		ss << "FPS: " << fps << " detection time: " << time * 1000 << " ms";
		putText(frame, ss.str(), Point(20, 20), 0, 0.5, Scalar(0, 255, 0));
		imshow("YOLOV4-Detection", frame);
		char c = waitKey(1);
		if (c == 27)
			break;
	}
	waitKey(0);
	return;
}
```

### 15.人脸检测

```c++
string objNames[] = {
	"background",
	"aeroplane", "bicycle", "bird", "boat",
	"bottle", "bus", "car", "cat", "chair",
	"cow", "diningtable", "dog", "horse",
	"motorbike", "person", "pottedplant",
	"sheep", "sofa", "train", "tvmonitor"
};

int main(int argc, char** argv) {
	std::string config = "D:/project/vsworkspaces/opencv4_feature_tutorial/dnn_face_detect_demo/deploy.prototxt";
	string model = "D:/project/vsworkspaces/opencv4_feature_tutorial/dnn_face_detect_demo/res10_300x300_ssd_iter_140000.caffemodel";
	Net net = readNetFromCaffe(config, model);
	net.setPreferableBackend(DNN_BACKEND_OPENCV);
	net.setPreferableTarget(DNN_TARGET_CPU);
	VideoCapture cap(0);
	Mat frame;
	while (true) {
		int64 start = getTickCount();
		bool ret = cap.read(frame);
		// 1表示垂直翻转,0表示水平翻转，-1表示水平和垂直都翻转
		flip(frame, frame, 1);
		if (!ret)
			break;
		// blobimage分析就是对前景/背景分离后的二值图像
		// 图像预处理, 整体像素值减去平均值, 通过缩放系数（scalefactor）对图片像素值进行缩放
		Mat blobImage = blobFromImage(frame, 1.0, Size(300, 300), Scalar(104, 177, 123), false, false);
		net.setInput(blobImage);
		// 是一个四维的返回值
		// 在最后一维，第二个开始依次是：标签、置信度、目标位置的4个坐标信息[xmin ymin xmax ymax]
		// 倒数第二维是检测到结果的索引
		Mat detection = net.forward();
		// 
		Mat detectionMat(detection.size[2], detection.size[3], CV_32F, detection.ptr<float>());
		float confidence_threshold = 0.5;
		for (int i = 0; i < detectionMat.rows; i++) {
			float* curr_row = detectionMat.ptr<float>(i);
			int image_id = (int)(*curr_row++);
			size_t objIndex = (size_t)(*curr_row++);
			float score = *curr_row++;
			if (score > confidence_threshold) {
				float t1_x = (*curr_row++) * frame.cols;
				float t1_y = (*curr_row++) * frame.rows;
				float br_x = (*curr_row++) * frame.cols;
				float br_y = (*curr_row++) * frame.rows;
				cout << "point: " << t1_x << "  " << t1_y <<"  " << (br_x - t1_x)<<"  " << (br_y - t1_y) << endl;
				Rect object_box((int)t1_x, (int)t1_y, (int)(br_x - t1_x), (int)(br_y - t1_y));
				rectangle(frame, object_box, Scalar(0, 0, 255), 2, 8, 0);
				putText(frame, format(" confidence %.2f, %s", score, objNames[objIndex].c_str()), Point(t1_x - 10, t1_y - 10), FONT_HERSHEY_SIMPLEX, 0.7, Scalar(255, 0, 0), 2, 8);
			}
		}
		float fps = getTickFrequency() / (getTickCount() - start);
		float time = (getTickCount() - start) / getTickFrequency();
		ostringstream ss;
		ss << "FPS: " << fps << " detection time: " << time * 1000 << " ms";
		putText(frame, ss.str(), Point(20, 20), 0, 0.5, Scalar(0, 0, 255));
		imshow("人脸检测", frame);
		char c = waitKey(1);
		if (c == 27)
			break;
	}
	destroyAllWindows();
	return 0;
}
```

